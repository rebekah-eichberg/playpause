{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:17:13.764716Z",
     "iopub.status.busy": "2025-08-01T01:17:13.764426Z",
     "iopub.status.idle": "2025-08-01T01:17:29.499850Z",
     "shell.execute_reply": "2025-08-01T01:17:29.498796Z",
     "shell.execute_reply.started": "2025-08-01T01:17:13.764687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:17:29.501066Z",
     "iopub.status.busy": "2025-08-01T01:17:29.500835Z",
     "iopub.status.idle": "2025-08-01T01:17:43.904232Z",
     "shell.execute_reply": "2025-08-01T01:17:43.903306Z",
     "shell.execute_reply.started": "2025-08-01T01:17:29.501040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install latest transformers for Gemma 3N\n",
    "!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:17:43.906800Z",
     "iopub.status.busy": "2025-08-01T01:17:43.906568Z",
     "iopub.status.idle": "2025-08-01T01:19:33.583220Z",
     "shell.execute_reply": "2025-08-01T01:19:33.582502Z",
     "shell.execute_reply.started": "2025-08-01T01:17:43.906777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 01:17:56.538441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754011076.755290      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754011076.813921      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.11: Fast Gemma3N patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be699b48828b4289bf4360fcbb79e698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e475e755d67a4daba983d253a9fcde5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93487e758cb54ce689219da70e8a3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b39280ac28402da2c98474de4ac0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c136e1970442cdbb9eb7ddcd2b505e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f526312863a42e7b120e3747db42767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b80b814e5b48d4b944a0ff70db97a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4449aa004e4b434da636f5758ba7edb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c537fa94284439afdcabb4626d1063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02449853b1984ee0aeccc1b6824d1727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de66a4c74b9941128a7bef4ddda52938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0004a3a02ca54bdfac6c29972b1a4d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d46252c1864239955328e43e7c8627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\", # Or \"unsloth/gemma-3n-E2B-it\"\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:19:33.584851Z",
     "iopub.status.busy": "2025-08-01T01:19:33.584171Z",
     "iopub.status.idle": "2025-08-01T01:19:33.591272Z",
     "shell.execute_reply": "2025-08-01T01:19:33.590624Z",
     "shell.execute_reply.started": "2025-08-01T01:19:33.584820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import gc\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(model, messages, max_new_tokens = 128):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )\n",
    "    # Cleanup to reduce VRAM usage\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:21:56.768552Z",
     "iopub.status.busy": "2025-08-01T01:21:56.768203Z",
     "iopub.status.idle": "2025-08-01T01:22:05.122250Z",
     "shell.execute_reply": "2025-08-01T01:22:05.121638Z",
     "shell.execute_reply.started": "2025-08-01T01:21:56.768532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:22:05.123253Z",
     "iopub.status.busy": "2025-08-01T01:22:05.123051Z",
     "iopub.status.idle": "2025-08-01T01:22:05.128152Z",
     "shell.execute_reply": "2025-08-01T01:22:05.127422Z",
     "shell.execute_reply.started": "2025-08-01T01:22:05.123237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:22:36.843967Z",
     "iopub.status.busy": "2025-08-01T01:22:36.843627Z",
     "iopub.status.idle": "2025-08-01T01:22:40.707687Z",
     "shell.execute_reply": "2025-08-01T01:22:40.706801Z",
     "shell.execute_reply.started": "2025-08-01T01:22:36.843946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture \n",
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:22:48.234210Z",
     "iopub.status.busy": "2025-08-01T01:22:48.233845Z",
     "iopub.status.idle": "2025-08-01T01:22:48.480663Z",
     "shell.execute_reply": "2025-08-01T01:22:48.479776Z",
     "shell.execute_reply.started": "2025-08-01T01:22:48.234179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available topics:\n",
      "- Human Body & Health: 60 examples\n",
      "- Animals & Nature: 14 examples\n",
      "- Earth & Space: 44 examples\n",
      "- Physics & Chemistry: 22 examples\n",
      "- Technology: 13 examples\n",
      "- Food & Nutrition: 9 examples\n",
      "- Language & Culture: 6 examples\n",
      "- Math & Logic: 2 examples\n",
      "- Sports & Recreation: 8 examples\n",
      "- Time & Holidays: 14 examples\n",
      "- Places & Geography: 11 examples\n",
      "- Myths & Legends: 3 examples\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Load document ===\n",
    "doc = Document(\"/kaggle/input/age-8-10/Ages 8-10.docx\")  # Update this path\n",
    "\n",
    "# === Setup ===\n",
    "topics = defaultdict(list)\n",
    "current_topic = None\n",
    "current_question = None\n",
    "current_answer = []\n",
    "\n",
    "# Topics start with capital letters and possibly \"&\"\n",
    "def is_topic_header(text):\n",
    "    return bool(re.match(r\"^[A-Z][a-zA-Z& ]+$\", text)) and not text.endswith(\"?\")\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = text.replace(\"Sparky's Answer:\", \"\").strip()\n",
    "    text = re.split(r\"Wow! Fact:.*?Wow!$\", text, flags=re.DOTALL)[0].strip()\n",
    "    return text\n",
    "\n",
    "# === Main parse loop ===\n",
    "for para in doc.paragraphs:\n",
    "    text = para.text.strip()\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    # Section/topic header\n",
    "    if is_topic_header(text):\n",
    "        current_topic = text\n",
    "\n",
    "    # Question\n",
    "    elif text.endswith(\"?\") and \"Sparky's Answer\" not in text:\n",
    "        if current_question and current_answer:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": current_question},\n",
    "                {\"role\": \"assistant\", \"content\": clean_answer(\" \".join(current_answer))}\n",
    "            ]\n",
    "            topics[current_topic].append({\"messages\": messages})\n",
    "            current_answer = []\n",
    "        current_question = text\n",
    "\n",
    "    # Answer continuation\n",
    "    elif \"Sparky's Answer:\" in text or current_answer:\n",
    "        current_answer.append(text)\n",
    "\n",
    "# Save last Q&A\n",
    "if current_question and current_answer:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": current_question},\n",
    "        {\"role\": \"assistant\", \"content\": clean_answer(\" \".join(current_answer))}\n",
    "    ]\n",
    "    topics[current_topic].append({\"messages\": messages})\n",
    "\n",
    "# === Outputs ===\n",
    "# 1. Flat list for fine-tuning\n",
    "finetune_data = [ex for topic in topics.values() for ex in topic]\n",
    "\n",
    "# 2. Show available topics and counts\n",
    "print(\"Available topics:\")\n",
    "for topic, examples in topics.items():\n",
    "    print(f\"- {topic}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:23:07.168849Z",
     "iopub.status.busy": "2025-08-01T01:23:07.168063Z",
     "iopub.status.idle": "2025-08-01T01:23:07.205833Z",
     "shell.execute_reply": "2025-08-01T01:23:07.205312Z",
     "shell.execute_reply.started": "2025-08-01T01:23:07.168817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(finetune_data)\n",
    "\n",
    "# Optional: create a train/test split\n",
    "dataset_split = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:23:10.617126Z",
     "iopub.status.busy": "2025-08-01T01:23:10.616803Z",
     "iopub.status.idle": "2025-08-01T01:23:10.625497Z",
     "shell.execute_reply": "2025-08-01T01:23:10.624566Z",
     "shell.execute_reply.started": "2025-08-01T01:23:10.617105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:23:22.442947Z",
     "iopub.status.busy": "2025-08-01T01:23:22.442672Z",
     "iopub.status.idle": "2025-08-01T01:23:22.449328Z",
     "shell.execute_reply": "2025-08-01T01:23:22.448505Z",
     "shell.execute_reply.started": "2025-08-01T01:23:22.442929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'What is the thermosphere?', 'role': 'user'},\n",
       "  {'content': 'The thermosphere is like the \"super-hot, top layer\" of Earth\\'s atmosphere, way, way up high! It\\'s where the air is very, very thin, but it gets super hot because it\\'s the first part of our atmosphere to absorb the sun\\'s energy. Even though it\\'s hot, you wouldn\\'t feel warm there because the air particles are so far apart!',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:35.181754Z",
     "iopub.status.busy": "2025-08-01T01:24:35.181101Z",
     "iopub.status.idle": "2025-08-01T01:24:35.671548Z",
     "shell.execute_reply": "2025-08-01T01:24:35.670783Z",
     "shell.execute_reply.started": "2025-08-01T01:24:35.181731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e23e3f0dd44cf3a9d8cae6bef6322b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"messages\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:50.053492Z",
     "iopub.status.busy": "2025-08-01T01:24:50.053182Z",
     "iopub.status.idle": "2025-08-01T01:24:50.059383Z",
     "shell.execute_reply": "2025-08-01T01:24:50.058544Z",
     "shell.execute_reply.started": "2025-08-01T01:24:50.053469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nWhat is the thermosphere?<end_of_turn>\\n<start_of_turn>model\\nThe thermosphere is like the \"super-hot, top layer\" of Earth\\'s atmosphere, way, way up high! It\\'s where the air is very, very thin, but it gets super hot because it\\'s the first part of our atmosphere to absorb the sun\\'s energy. Even though it\\'s hot, you wouldn\\'t feel warm there because the air particles are so far apart!<end_of_turn>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:25:41.294675Z",
     "iopub.status.busy": "2025-08-01T01:25:41.294316Z",
     "iopub.status.idle": "2025-08-01T01:25:45.335075Z",
     "shell.execute_reply": "2025-08-01T01:25:45.334160Z",
     "shell.execute_reply.started": "2025-08-01T01:25:41.294651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6749db3adf4e0789a217b8bc92cf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resume a training run, set trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:26:04.927486Z",
     "iopub.status.busy": "2025-08-01T01:26:04.926912Z",
     "iopub.status.idle": "2025-08-01T01:26:05.607968Z",
     "shell.execute_reply": "2025-08-01T01:26:05.607028Z",
     "shell.execute_reply.started": "2025-08-01T01:26:04.927453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490f1cecf70b4da0849400b48d7ece6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:26:30.516073Z",
     "iopub.status.busy": "2025-08-01T01:26:30.515217Z",
     "iopub.status.idle": "2025-08-01T01:26:30.523101Z",
     "shell.execute_reply": "2025-08-01T01:26:30.522431Z",
     "shell.execute_reply.started": "2025-08-01T01:26:30.516042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the thermosphere?<end_of_turn>\\n<start_of_turn>model\\nThe thermosphere is like the \"super-hot, top layer\" of Earth\\'s atmosphere, way, way up high! It\\'s where the air is very, very thin, but it gets super hot because it\\'s the first part of our atmosphere to absorb the sun\\'s energy. Even though it\\'s hot, you wouldn\\'t feel warm there because the air particles are so far apart!<end_of_turn>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:26:51.239684Z",
     "iopub.status.busy": "2025-08-01T01:26:51.239351Z",
     "iopub.status.idle": "2025-08-01T01:26:51.246670Z",
     "shell.execute_reply": "2025-08-01T01:26:51.245933Z",
     "shell.execute_reply.started": "2025-08-01T01:26:51.239662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'               The thermosphere is like the \"super-hot, top layer\" of Earth\\'s atmosphere, way, way up high! It\\'s where the air is very, very thin, but it gets super hot because it\\'s the first part of our atmosphere to absorb the sun\\'s energy. Even though it\\'s hot, you wouldn\\'t feel warm there because the air particles are so far apart!<end_of_turn>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:27:44.434837Z",
     "iopub.status.busy": "2025-08-01T01:27:44.434483Z",
     "iopub.status.idle": "2025-08-01T01:35:40.901985Z",
     "shell.execute_reply": "2025-08-01T01:35:40.901338Z",
     "shell.execute_reply.started": "2025-08-01T01:27:44.434816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 206 | Num Epochs = 3 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 19,210,240 of 7,869,188,432 (0.24% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>11.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>12.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>12.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>13.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>11.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>12.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>12.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>11.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.649300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:41:22.494288Z",
     "iopub.status.busy": "2025-08-01T01:41:22.493944Z",
     "iopub.status.idle": "2025-08-01T01:41:55.976731Z",
     "shell.execute_reply": "2025-08-01T01:41:55.975869Z",
     "shell.execute_reply.started": "2025-08-01T01:41:22.494269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<bos><start_of_turn>user\\nHow do birds know where to fly when they migrate?<end_of_turn>\\n<start_of_turn>model\\nBirds have amazing navigation skills! They use a combination of things to know where to fly during migration:\\n\\n* **The Sun:** They can tell which direction is east and west using the sun's position.\\n* **The Stars:** At night, they use the stars to guide them.\\n* **The Earth\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"How do birds know where to fly when they migrate?\",\n",
    "    }]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:42:43.650698Z",
     "iopub.status.busy": "2025-08-01T01:42:43.650351Z",
     "iopub.status.idle": "2025-08-01T01:42:48.353884Z",
     "shell.execute_reply": "2025-08-01T01:42:48.353097Z",
     "shell.execute_reply.started": "2025-08-01T01:42:43.650674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma-3n/processor_config.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3n\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3n\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7984878,
     "sourceId": 12636371,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
