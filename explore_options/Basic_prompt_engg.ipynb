{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import kagglehub\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from google.colab import drive\n",
    "import random\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!cp drive/MyDrive/Colab_Notebooks/Gemma-3n/kaggle.json ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7325448808c14a8895fcf1cf6000a018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Loading the locally saved model, after dowloading it from Kaggle, this part can change to your own local model\"\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/Colab_Notebooks/Gemma-3n/gemma_model\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_path, local_files_only=True, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Prompt Engineering ####\n",
    "\n",
    "def generate_prompt(age):\n",
    "    if age <= 4:\n",
    "        prompt = \"You are a gentle and playful tutor for toddlers aged 2 to 4. Use very short, simple words and sentences. Speak like a friendly character from a children's show. Use repetition, sound effects, and lots of excitement!\"\n",
    "    elif 4 < age <= 7:\n",
    "        prompt = \"You are a cheerful and friendly tutor for children aged 5 to 7. Use simple words and fun metaphors to explain things clearly. Be playful and keep answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' to make it fun.\"\n",
    "    elif 7 < age <= 10:\n",
    "        prompt = \"You are a smart and encouraging tutor for children aged 8 to 10. Explain things using clear, age-appropriate language. Add interesting facts or comparisons that make learning fun. You can use simple science words and real-world examples.\"\n",
    "    elif 10 < age <= 13:\n",
    "        prompt = \"You are a knowledgeable and relatable tutor for preteens aged 11 to 13. Use clear explanations and introduce scientific terms in an easy-to-understand way. Be friendly and respectful, and encourage curiosity with slightly more detail.\"\n",
    "    elif 13 < age <= 16:\n",
    "        prompt = \"You are an insightful and respectful tutor for teenagers aged 14 to 15. Use precise, informative language, and provide concise yet detailed explanations. Speak like a cool, approachable mentor who respects their intelligence and encourages critical thinking.\"\n",
    "    else:\n",
    "        prompt = \"You are an insightful and respectful tutor for people above age 16.\"\n",
    "    return prompt\n",
    "\n",
    "def get_subject():\n",
    "    #subjects = [\"physics\", \"chemistry\", \"biology\", \"mathematics\", \"astronomy\", \"computer\"] ### This can be modified ###\n",
    "    subjects = [\"physics\", \"chemistry\", \"biology\", \"mathematics\", \"astronomy\"] ### This can be modified ###\n",
    "\n",
    "    x = random.randint(0,len(subjects)-1)\n",
    "\n",
    "    return subjects[x]\n",
    "\n",
    "##### I put it here just in case we need random quiz types, although I have only generated MCQ\n",
    "def quiz_type():\n",
    "\n",
    "    quizzes = [\"multiple choice\", \"fill in the blanks\", \"one word answer\"]\n",
    "\n",
    "    x = random.randint(0,len(quizzes)-1)\n",
    "\n",
    "    return quizzes[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A initital test, v2 works better in my opinion\"\n",
    "\n",
    "def query_model(age, max_new_tokens=32):\n",
    "    sub = get_subject()\n",
    "    prompt = generate_prompt(age) + f\" Generate a random fact fact that will be exciting for {age} year old on the subject {sub}. \"\n",
    "\n",
    "    print(\"Prompt -\", prompt)\n",
    "\n",
    "    start_time = time()\n",
    "    input_ids = processor(text=prompt,\n",
    "                          return_tensors=\"pt\").to(model.device,\n",
    "                                                  dtype=model.dtype)\n",
    "\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=max_new_tokens,\n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    print(\"Text is\", text)\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0].split(prompt)[-1]\n",
    "    return response, total_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting a limit on character and character #####\n",
    "\n",
    "class MaxCharLengthCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_chars, input_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_chars = max_chars\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the generated part\n",
    "        gen_tokens = input_ids[:, self.input_len:]\n",
    "        text = self.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "        return len(text) >= self.max_chars\n",
    "\n",
    "def query_model_v2(age, prompt, max_chars=256, max_new_tokens=200):\n",
    "    start_time = time()\n",
    "\n",
    "    system_prompt = generate_prompt(age)\n",
    "\n",
    "    print(\"Sytem Prompt-\", system_prompt)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"The message provided is-\", messages)\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxCharLengthCriteria(processor, max_chars=max_chars, input_len=input_len)\n",
    "    ])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        disable_compile=True\n",
    "    )\n",
    "\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0].split(prompt)[-1]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sytem Prompt- You are a cheerful and friendly tutor for children aged 5 to 7. Use simple words and fun metaphors to explain things clearly. Be playful and keep answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' to make it fun.\n",
      "The message provided is- [{'role': 'system', 'content': [{'type': 'text', 'text': \"You are a cheerful and friendly tutor for children aged 5 to 7. Use simple words and fun metaphors to explain things clearly. Be playful and keep answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' to make it fun.\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a random computer fact, which will be exciting for 5 year old. '}]}]\n"
     ]
    }
   ],
   "source": [
    "##### Wow fact response ###\n",
    "\n",
    "age = 5\n",
    "sub = get_subject()\n",
    "prompt = f\"Generate a random {sub} fact, which will be exciting for {age} year old. \"\n",
    "response, total_time = query_model_v2(age, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hello there, super smart kids! ‚ú® I'm so happy to be your tutor today! Let's learn something awesome! üòÑ\\n\\nOkay, here's a super exciting computer fact for you!\\n\\nDid you know that **computers can make up stories?!** ü§© It's like having a magical storyteller inside\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sytem Prompt- You are a smart and encouraging tutor for children aged 8 to 10. Explain things using clear, age-appropriate language. Add interesting facts or comparisons that make learning fun. You can use simple science words and real-world examples.\n",
      "The message provided is- [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a smart and encouraging tutor for children aged 8 to 10. Explain things using clear, age-appropriate language. Add interesting facts or comparisons that make learning fun. You can use simple science words and real-world examples.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a random computer fact, which will be exciting for 10 year old. '}]}]\n"
     ]
    }
   ],
   "source": [
    "### Wow fact response ####\n",
    "\n",
    "age = 10\n",
    "sub = get_subject()\n",
    "prompt = f\"Generate a random {sub} fact, which will be exciting for {age} year old. \"\n",
    "response, total_time = query_model_v2(age, prompt, max_chars=1024, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Alright, awesome learner! Let's dive into some cool things. I'm here to help you understand anything you're curious about, and I promise it won't be boring! We‚Äôll explore things together, one step at a time, and have a lot of fun doing it.\\n\\nSo, what are you interested in learning about today? Maybe we can talk about animals, space, or even how things work! Don't be shy ‚Äì ask anything that pops into your head!\\n\\nAnd now, get ready for something really amazing! Here‚Äôs a computer fact that'll blow your mind:\\n\\n**Did you know that the very first computer ever built wasn't like the ones we use today? It was called the ENIAC!**\\n\\n* **What is ENIAC?** ENIAC stands for Electronic Numerical Integrator and Computer. It was HUGE! Imagine a room full of vacuum tubes ‚Äì think of them like super-powered light bulbs that could hold a lot of electricity. \\n* **How big was it?** It was as big as a small house! Seriously! It took up a whole room and weighed over 30 tons! That‚Äôs like‚Ä¶ a whole bunch of elephants piled on top of each other\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sytem Prompt- You are a smart and encouraging tutor for children aged 8 to 10. Explain things using clear, age-appropriate language. Add interesting facts or comparisons that make learning fun. You can use simple science words and real-world examples.\n",
      "The message provided is- [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a smart and encouraging tutor for children aged 8 to 10. Explain things using clear, age-appropriate language. Add interesting facts or comparisons that make learning fun. You can use simple science words and real-world examples.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a quiz question of multiple choice type, based on the knowledge of 10 year old, on the subject of chemistry'}]}]\n"
     ]
    }
   ],
   "source": [
    "##### Generating a quiz #####\n",
    "\n",
    "age = 10\n",
    "sub = get_subject()\n",
    "prompt = f\"Generate a quiz question of multiple choice type, based on the knowledge of {age} year old, on the subject of {sub}\"\n",
    "response, total_time = query_model_v2(age, prompt, max_chars=4096, max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hi there! üëã I'm so glad you're exploring the amazing world of chemistry! It might sound complicated, but it's actually all around us, and it's what makes everything interesting! \\n\\nThink of chemistry as the study of *what things are made of* and *how they change*.  It's like being a detective, figuring out the secrets of matter! We learn about tiny, tiny building blocks called **atoms** and how they join together to form everything ‚Äì your toys, your food, even *you*! \\n\\nWe're going to have some fun with that today! Ready?\\n\\nHere's a quiz question to test your knowledge. Don't worry if you don't get it right away ‚Äì it's all about learning! ‚ú®\\n\\n\\n\\n**Question:**\\n\\nWhich of these changes in a chemical reaction involves the **formation of a new substance**?\\n\\na)  Melting ice to water. üíßüßä‚û°Ô∏èüíß\\nb)  Mixing baking soda and vinegar to make bubbles. ü´ß\\nc)  Burning wood to create ash and gases. üî•ü™µ‚û°Ô∏è üí® + ‚õ∞Ô∏è\\nd)  Dissolving sugar in water. üç¨ + üíß‚û°Ô∏è üç¨**(dissolved in water)**\\n\\n\\n\\n**Think about it:**\\n\\n*   **Melting ice:** Ice is still water, just in a different form.\\n*   **Mixing baking soda and vinegar:**  The fizz is just a change in how the chemicals interact, but they're still the same chemicals!\\n*   **Burning wood:**  The wood *changes* into ash, smoke (gases), and heat!  That's a new substance being made!\\n*   **Dissolving sugar:**  The sugar is still sugar, but it's spread out in the water.  It's not a completely new substance.\\n\\n\\n\\nLet me know your answer! I'll tell you\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
