{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12673548,"sourceType":"datasetVersion","datasetId":8009082}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:25:04.688698Z","iopub.execute_input":"2025-08-04T23:25:04.688888Z","iopub.status.idle":"2025-08-04T23:25:22.582674Z","shell.execute_reply.started":"2025-08-04T23:25:04.688870Z","shell.execute_reply":"2025-08-04T23:25:22.581529Z"},"id":"lR3QCP_OPx1g"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n# Install latest transformers for Gemma 3N\n!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:25:22.583830Z","iopub.execute_input":"2025-08-04T23:25:22.584101Z","iopub.status.idle":"2025-08-04T23:25:40.415652Z","shell.execute_reply.started":"2025-08-04T23:25:22.584071Z","shell.execute_reply":"2025-08-04T23:25:40.414662Z"},"id":"mdetXTiMPx1i"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:25:40.417806Z","iopub.execute_input":"2025-08-04T23:25:40.418049Z","iopub.status.idle":"2025-08-04T23:25:40.422750Z","shell.execute_reply.started":"2025-08-04T23:25:40.418024Z","shell.execute_reply":"2025-08-04T23:25:40.421981Z"},"id":"9ELuVvaCPx1i"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:25:40.423673Z","iopub.execute_input":"2025-08-04T23:25:40.423918Z","iopub.status.idle":"2025-08-04T23:25:46.843706Z","shell.execute_reply.started":"2025-08-04T23:25:40.423891Z","shell.execute_reply":"2025-08-04T23:25:46.843029Z"},"id":"KjLMnqlMPx1j"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n    # Pretrained models\n    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n\n    # Other Gemma 3 quants\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E4B-it\"\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = BASE_MODEL_NAME, # Or \"unsloth/gemma-3n-E2B-it\"\n    dtype = None, # None for auto detection\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:25:46.844499Z","iopub.execute_input":"2025-08-04T23:25:46.844851Z","iopub.status.idle":"2025-08-04T23:27:50.035981Z","shell.execute_reply.started":"2025-08-04T23:25:46.844812Z","shell.execute_reply":"2025-08-04T23:27:50.035055Z"},"colab":{"referenced_widgets":["af1f64ac410d40d8975bab6edbe694a0","e3a5baf7b3194d32b354d4049d51db1c","b2ea277de1f54d5c80cdd7cf58de4e9c","18409252f2c64dea9ebfa6df04bd3ef9","1a555b0439434600a860e64a4b8f9aa3","239d83fc78134269bdc09b2ed5b71af1","383947ef12bd41fc91e83d78c3c31727","b3260ee0399146bfa7b8a6b02a9eb124","380c4591023f49a3b7a0d2b6a248f941","92976339c4ba4f3f9e688aab52caa7d9","ddb8fe1984dd44afb5b04f7fe3347450","168368f691494ef99e3c5251897a2505","f6f98c3e0bdf4156b559c771ce99db41"]},"id":"jLv4h3zwPx1j","outputId":"3c03ed8d-e23e-4211-e11d-7194ba284e5d"},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-04 23:25:58.320968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754349958.668792      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754349958.767037      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f43078c80d046559018937112ba699f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ae137c16cf4a4182fa75176eb90fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd4640eff9f45e9ac73f1372660d287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd2af5f6a9c4824b6d442643a87e53b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5f5676b9f74d0198afa59b93aed395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ee8dd46ff0b4c8ca1a3cf64cf3698be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e4af53b5b54c3ba14e65efba14bba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758a6edea4d848f694f4e5bdab5c4f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031e68b8ccec4397b69fcb920d5e4a44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac8a34a34d64a2597fead982b6d8b44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb756b2613104b8f87aac4b9e73d74e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae1289da72e4a27ba4f45953a4f075f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331ceaab0ad24636b426b119804db194"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import TextStreamer\nimport gc\n# Helper function for inference\ndef do_gemma_3n_inference(model, messages, max_new_tokens = 128):\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt = True, # Must add for generation\n        tokenize = True,\n        return_dict = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n    _ = model.generate(\n        **inputs,\n        max_new_tokens = max_new_tokens,\n        temperature = 1.0, top_p = 0.95, top_k = 64,\n        streamer = TextStreamer(tokenizer, skip_prompt = True),\n    )\n    # Cleanup to reduce VRAM usage\n    del inputs\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:50.037094Z","iopub.execute_input":"2025-08-04T23:27:50.037425Z","iopub.status.idle":"2025-08-04T23:27:50.044777Z","shell.execute_reply.started":"2025-08-04T23:27:50.037395Z","shell.execute_reply":"2025-08-04T23:27:50.043640Z"},"id":"KNv_IPQ5Px1k"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:50.045927Z","iopub.execute_input":"2025-08-04T23:27:50.046313Z","iopub.status.idle":"2025-08-04T23:27:50.072199Z","shell.execute_reply.started":"2025-08-04T23:27:50.046283Z","shell.execute_reply":"2025-08-04T23:27:50.071184Z"},"id":"9LVb04YpPx1k"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%capture\npip install python-docx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:50.073180Z","iopub.execute_input":"2025-08-04T23:27:50.073480Z","iopub.status.idle":"2025-08-04T23:27:54.505054Z","shell.execute_reply.started":"2025-08-04T23:27:50.073457Z","shell.execute_reply":"2025-08-04T23:27:54.503780Z"},"id":"FmJJPH6QPx1l"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# If not already installed in this environment; uncomment if needed\n# !pip install python-docx datasets transformers unsloth\n\nimport os\nimport re\nfrom collections import defaultdict\nfrom docx import Document\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:54.508940Z","iopub.execute_input":"2025-08-04T23:27:54.509273Z","iopub.status.idle":"2025-08-04T23:27:54.640204Z","shell.execute_reply.started":"2025-08-04T23:27:54.509244Z","shell.execute_reply":"2025-08-04T23:27:54.639280Z"},"id":"_mlxlMqBPx1l"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === CONFIG ===\nINPUT_DIR = \"/kaggle/input/raw-docs\"  # folder with your .docx files\nAGE_GROUP_REGEX = re.compile(r\"Ages?\\s*([\\d\\-]+)\", flags=re.IGNORECASE)\nSPLIT_RATIOS = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\nMAX_LENGTH = 256\nIGNORE_INDEX = -100\n\n# === Helpers ===\ndef infer_age_group_from_filename(path):\n    fname = os.path.basename(path)\n    m = AGE_GROUP_REGEX.search(fname)\n    return m.group(1) if m else \"unknown\"\n\ndef is_topic_header(text):\n    return bool(re.match(r\"^[A-Z][a-zA-Z& ]+$\", text)) and not text.endswith(\"?\")\n\ndef clean_answer(text):\n    text = text.replace(\"Sparky's Answer:\", \"\").strip()\n    return re.split(r\"Wow! Fact:.*?Wow!$\", text, flags=re.DOTALL)[0].strip()\n\ndef normalize_subject(subject):\n    if not subject:\n        return \"unknown\"\n    s = subject.strip().lower()\n    if \"math\" in s:\n        return \"math\"\n    if \"science\" in s:\n        return \"science\"\n    if \"geography\" in s:\n        return \"geography\"\n    if \"history\" in s:\n        return \"history\"\n    return s\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:54.641202Z","iopub.execute_input":"2025-08-04T23:27:54.642544Z","iopub.status.idle":"2025-08-04T23:27:54.652862Z","shell.execute_reply.started":"2025-08-04T23:27:54.642520Z","shell.execute_reply":"2025-08-04T23:27:54.651897Z"},"id":"solStJ3EPx1l"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def parse_docx_file(path):\n    print(f\"Parsing {path}\")\n    doc = Document(path)\n    age_group = infer_age_group_from_filename(path)\n    examples = []\n    current_topic = None\n    current_question = None\n    current_answer_parts = []\n\n    for para in doc.paragraphs:\n        text = para.text.strip()\n        if not text:\n            continue\n        if is_topic_header(text):\n            current_topic = text\n        elif text.endswith(\"?\") and \"Sparky's Answer\" not in text:\n            if current_question and current_answer_parts:\n                cleaned = clean_answer(\" \".join(current_answer_parts))\n                examples.append({\n                    \"question\": current_question,\n                    \"answer\": cleaned,\n                    \"subject\": normalize_subject(current_topic),\n                    \"age_group\": age_group,\n                    \"format\": \"open_ended\",\n                })\n                current_answer_parts = []\n            current_question = text\n        elif \"Sparky's Answer:\" in text or current_answer_parts:\n            current_answer_parts.append(text)\n\n    if current_question and current_answer_parts:\n        cleaned = clean_answer(\" \".join(current_answer_parts))\n        examples.append({\n            \"question\": current_question,\n            \"answer\": cleaned,\n            \"subject\": normalize_subject(current_topic),\n            \"age_group\": age_group,\n            \"format\": \"open_ended\",\n        })\n\n    print(f\"  → extracted {len(examples)} examples\")\n    return examples\n\n# Collect all examples by age group\nby_age = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if fname.lower().endswith(\".docx\"):\n            path = os.path.join(root, fname)\n            exs = parse_docx_file(path)\n            for ex in exs:\n                by_age[ex[\"age_group\"]].append(ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:54.654213Z","iopub.execute_input":"2025-08-04T23:27:54.654563Z","iopub.status.idle":"2025-08-04T23:27:55.042052Z","shell.execute_reply.started":"2025-08-04T23:27:54.654533Z","shell.execute_reply":"2025-08-04T23:27:55.040944Z"},"id":"_swfSYlXPx1m","outputId":"c5e2c806-6594-4f7c-de34-e7c3794025b4"},"outputs":[{"name":"stdout","text":"Parsing /kaggle/input/raw-docs/Ages 14-15.docx\n  → extracted 15 examples\nParsing /kaggle/input/raw-docs/Ages 11-13.docx\n  → extracted 49 examples\nParsing /kaggle/input/raw-docs/Ages 5-7.docx\n  → extracted 103 examples\nParsing /kaggle/input/raw-docs/Ages 8-10.docx\n  → extracted 206 examples\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from copy import deepcopy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:55.043026Z","iopub.execute_input":"2025-08-04T23:27:55.043396Z","iopub.status.idle":"2025-08-04T23:27:55.048484Z","shell.execute_reply.started":"2025-08-04T23:27:55.043362Z","shell.execute_reply":"2025-08-04T23:27:55.047433Z"},"id":"hzqddd1oPx1o"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Build raw_datasets_by_age (stratified splits)\nby_age_raw = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if not fname.lower().endswith(\".docx\"):\n            continue\n        path = os.path.join(root, fname)\n        examples = parse_docx_file(path)  # your existing parser\n        for ex in examples:\n            by_age_raw[ex[\"age_group\"]].append(ex)\n\nraw_datasets_by_age = {}\nfor age_group, examples in by_age_raw.items():\n    ds = Dataset.from_list(examples)\n    try:\n        train_testval = ds.train_test_split(\n            test_size=1 - SPLIT_RATIOS[\"train\"],\n            seed=42,\n            stratify_by_column=\"subject\",\n        )\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n            stratify_by_column=\"subject\",\n        )\n        print(f\"-- Stratified succesfully for Age group {age_group}.\\n\")\n    except Exception:\n        train_testval = ds.train_test_split(test_size=1 - SPLIT_RATIOS[\"train\"], seed=42)\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n        )\n        print(f\"-- Stratification failed for Age group {age_group}.\\n\")\n\n    raw_datasets_by_age[age_group] = DatasetDict({\n        \"train\": train_testval[\"train\"],\n        \"val\": val_test[\"train\"],\n        \"test\": val_test[\"test\"],\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:55.049656Z","iopub.execute_input":"2025-08-04T23:27:55.049931Z","iopub.status.idle":"2025-08-04T23:27:55.480633Z","shell.execute_reply.started":"2025-08-04T23:27:55.049912Z","shell.execute_reply":"2025-08-04T23:27:55.479689Z"},"id":"1T3Y59fXPx1o","outputId":"78b17900-fb2d-486f-9615-4f7d579f889c"},"outputs":[{"name":"stdout","text":"Parsing /kaggle/input/raw-docs/Ages 14-15.docx\n  → extracted 15 examples\nParsing /kaggle/input/raw-docs/Ages 11-13.docx\n  → extracted 49 examples\nParsing /kaggle/input/raw-docs/Ages 5-7.docx\n  → extracted 103 examples\nParsing /kaggle/input/raw-docs/Ages 8-10.docx\n  → extracted 206 examples\n-- Stratification failed for Age group 14-15.\n\n-- Stratification failed for Age group 11-13.\n\n-- Stratification failed for Age group 5-7.\n\n-- Stratification failed for Age group 8-10.\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def system_prompt_for_age(age_group):\n  if age_group == \"2-4\":\n    system_prompt = (\"You are a gentle and playful tutor for toddlers aged 2 to 4.\"\n        \"Use very short, simple words and sentences. Speak like a friendly character \"\n        \"from a children's show. Use repetition, sound effects, and lots of excitement!\")\n  elif age_group == \"5-7\":\n    system_prompt  = (\"You are a cheerful and friendly tutor for children aged 5 to 7.\"\n    \" Use simple words and fun metaphors to explain things clearly. Be playful and keep \"\n    \"answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' \"\n    \"to make it fun.\")\n  elif age_group == \"8-10\":\n    system_prompt  = (\"You are a smart and encouraging tutor for children aged 8 to 10.\"\n    \" Explain things using clear, age-appropriate language. Add interesting facts or \"\n    \"comparisons that make learning fun. You can use simple science words and \"\n    \"real-world examples.\")\n  elif age_group == \"11-13\":\n    system_prompt  = (\"You are a knowledgeable and relatable tutor for preteens aged 11 to 13.\"\n    \" Use clear explanations and introduce scientific terms in an easy-to-understand way. \"\n    \"Be friendly and respectful, and encourage curiosity with slightly more detail.\")\n  elif age_group == \"14-15\":\n    system_prompt  = (\"You are an insightful and respectful tutor for teenagers aged 14 to 15. \"\n    \"Use precise, informative language, and provide concise yet detailed explanations. \"\n    \"Speak like a cool, approachable mentor who respects their intelligence and encourages \"\n    \"critical thinking.\")\n  else:\n    system_prompt = (\"You are an insightful and respectful tutor for people above age 16.\")\n  return system_prompt\n","metadata":{"id":"2iDhsxUKTCd3","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:55.481681Z","iopub.execute_input":"2025-08-04T23:27:55.482025Z","iopub.status.idle":"2025-08-04T23:27:55.487574Z","shell.execute_reply.started":"2025-08-04T23:27:55.481998Z","shell.execute_reply":"2025-08-04T23:27:55.486715Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Add \"text\" field for SFTTrainer\ndef add_text_field(ds_dict, age_group):\n    def make_text(example):\n        convo = [\n            {\"role\": \"system\", \"content\": system_prompt_for_age(age_group)},\n            {\"role\": \"user\", \"content\": example[\"question\"]},\n            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n        ]\n        example[\"text\"] = tokenizer.apply_chat_template(\n            convo, tokenize=False, add_generation_prompt=True\n        ).removeprefix(\"<bos>\")\n        return example\n    return ds_dict.map(make_text, batched=False)\n\n# Combine datasets from all age groups\nfrom datasets import concatenate_datasets # Import concatenate_datasets\n# Combine datasets from all age groups\ncombined_dataset = DatasetDict()\nfor age_group, raw_ds in raw_datasets_by_age.items():\n    # Apply the modified add_text_field to each age group's dataset\n    processed_ds = add_text_field(raw_ds, age_group)\n    # Concatenate the splits\n    for split in [\"train\", \"val\", \"test\"]:\n        if split not in combined_dataset:\n            combined_dataset[split] = processed_ds[split]\n        else:\n            combined_dataset[split] = concatenate_datasets([combined_dataset[split], processed_ds[split]])\n\nds = combined_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:27:55.488600Z","iopub.execute_input":"2025-08-04T23:27:55.488905Z","iopub.status.idle":"2025-08-04T23:28:02.059436Z","shell.execute_reply.started":"2025-08-04T23:27:55.488879Z","shell.execute_reply":"2025-08-04T23:28:02.058465Z"},"colab":{"referenced_widgets":["7b20eae4c0c54e71ae7ba7cb06a70517","00f0e8fb5ad649ad8fdd85cce1043604","4658bccd7f154b7fb70e036b74b8011c","b863ac1518884857afa6fe74633a6578","99a8cb28070f4f309d5d7590e3e76c47","093d57a8d4e04d5aa1db526229b95e0e","a77470040037493b91384224bf3c86c0","0e05556489a340cc9941b6a1c34cc431","c7ebc6f9f9d142f989c9262d323cf59b","9c5df05edc9c4d88a19071582d9fb1ae","4a4de76a7b3e4e3481fcb23db7ea3cfc","53014dc441df4716bc203307f5ad2829"]},"id":"lcc5AvU9Px1p","outputId":"5f1fbc67-0c74-4b4d-f776-8f408ec365e6"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c8005bba7df473cb8b9e2e69b2f44ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96e965327564e0d858b064b3dfc5f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026b0441ad2d4d698286ddfb86606ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b98bf63201194e83b0629ebe4d623637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131538bae8dc476588ff82f6d1c921d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db796077621a477d814945d314ee2d8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2771e5e55b28480a8b1fb44e076b65fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3743c96eb645d09e9186ee88d1cdd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68ad284e901e40c5b9d1be0a42a1ff81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d453f171b246d883c6e65e653f2cde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e94882cdee14ef5b040dc7514780706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f782f9e0e44fa68a01f26f57949355"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"id":"5lUqnQOtt7IF","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:28:02.060452Z","iopub.execute_input":"2025-08-04T23:28:02.060824Z","iopub.status.idle":"2025-08-04T23:28:02.067828Z","shell.execute_reply.started":"2025-08-04T23:28:02.060795Z","shell.execute_reply":"2025-08-04T23:28:02.066727Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n12.592 GB of memory reserved.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\nfrom trl import SFTTrainer, SFTConfig\nimport torch\nimport copy\n\n# Load base model once (frozen except adapters)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Add LoRA adapters to the model\nmodel = FastModel.get_peft_model(\n    model,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0,\n    bias=\"none\",\n    use_cache = False,\n    use_gradient_checkpointing=True,  # True or \"unsloth\" for very long context\n    use_rslora=True,\n    random_state=73\n)\n\nmodel.print_trainable_parameters()\n\n# Create SFTTrainer for this age\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer= tokenizer,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds.get(\"val\"),\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=10,\n        optim=\"paged_adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=42,\n        report_to=\"none\",\n    ),\n)\n\nfrom unsloth import unsloth_train\ntrainer_stats = unsloth_train(trainer) # trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:28:02.069040Z","iopub.execute_input":"2025-08-04T23:28:02.069369Z","iopub.status.idle":"2025-08-04T23:42:06.909767Z","shell.execute_reply.started":"2025-08-04T23:28:02.069341Z","shell.execute_reply":"2025-08-04T23:42:06.908806Z"},"colab":{"referenced_widgets":["f730ce8b887a4cc09cf55228d04cdefa","039901c855584914b47835f92d35a80f","7dc94ecd42174c4a83557e1227b18912","b9ea857b2bb94b1c8c28bed6993be205","fbe8e78739764b248f69afc1ffdd9517","0b8fce4bfd4e4bf0b8a7df4c378e9a86","f90246db13334a54b3f930514518619e","ae618a4f30ee4759845bb7a0ecde6dae"]},"id":"tKQ96gjTPx1p","outputId":"5a67acde-65bf-4fa8-96ce-fc30610d137c","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model.language_model` require gradients\ntrainable params: 40,189,952 || all params: 7,890,168,144 || trainable%: 0.5094\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/297 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e922cfdc064749be2c63e51405c08a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/37 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad8c6d0994694818b1a38b750a25d918"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 297 | Num Epochs = 2 | Total steps = 100\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n \"-____-\"     Trainable parameters = 40,189,952 of 7,890,168,144 (0.51% trained)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 11:27, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>12.709400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>12.659500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.396600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.778700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>6.388000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>6.396100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>5.017800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.586800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>4.967800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.089400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"model_dir = f\"./saved_model\"\n#cleanup_directory(model_dir)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nmodel.save_pretrained_merged(model_dir, tokenizer, save_method=\"merged_4bit_forced\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:42:06.911102Z","iopub.execute_input":"2025-08-04T23:42:06.912352Z","iopub.status.idle":"2025-08-04T23:43:08.532256Z","shell.execute_reply.started":"2025-08-04T23:42:06.912309Z","shell.execute_reply":"2025-08-04T23:43:08.531218Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Unsloth: Merging LoRA weights into 4bit model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging finished.\nUnsloth: Found skipped modules: ['model.language_model.layers.0.altup.correction_coefs', 'model.language_model.layers.0.altup.prediction_coefs', 'model.language_model.layers.0.altup.modality_router', 'model.language_model.layers.0.per_layer_projection', 'model.language_model.layers.1.altup.correction_coefs', 'model.language_model.layers.1.altup.prediction_coefs', 'model.language_model.layers.1.altup.modality_router', 'model.language_model.layers.1.per_layer_projection', 'model.language_model.layers.2.altup.correction_coefs', 'model.language_model.layers.2.altup.prediction_coefs', 'model.language_model.layers.2.altup.modality_router', 'model.language_model.layers.2.per_layer_projection', 'model.language_model.layers.3.altup.correction_coefs', 'model.language_model.layers.3.altup.prediction_coefs', 'model.language_model.layers.3.altup.modality_router', 'model.language_model.layers.3.per_layer_projection', 'model.language_model.layers.4.altup.correction_coefs', 'model.language_model.layers.4.altup.prediction_coefs', 'model.language_model.layers.4.altup.modality_router', 'model.language_model.layers.4.per_layer_projection', 'model.language_model.layers.5.altup.correction_coefs', 'model.language_model.layers.5.altup.prediction_coefs', 'model.language_model.layers.5.altup.modality_router', 'model.language_model.layers.5.per_layer_projection', 'model.language_model.layers.6.altup.correction_coefs', 'model.language_model.layers.6.altup.prediction_coefs', 'model.language_model.layers.6.altup.modality_router', 'model.language_model.layers.6.per_layer_projection', 'model.language_model.layers.7.altup.correction_coefs', 'model.language_model.layers.7.altup.prediction_coefs', 'model.language_model.layers.7.altup.modality_router', 'model.language_model.layers.7.per_layer_projection', 'model.language_model.layers.8.altup.correction_coefs', 'model.language_model.layers.8.altup.prediction_coefs', 'model.language_model.layers.8.altup.modality_router', 'model.language_model.layers.8.per_layer_projection', 'model.language_model.layers.9.altup.correction_coefs', 'model.language_model.layers.9.altup.prediction_coefs', 'model.language_model.layers.9.altup.modality_router', 'model.language_model.layers.9.per_layer_projection', 'model.language_model.layers.10.altup.correction_coefs', 'model.language_model.layers.10.altup.prediction_coefs', 'model.language_model.layers.10.altup.modality_router', 'model.language_model.layers.10.per_layer_projection', 'model.language_model.layers.11.altup.correction_coefs', 'model.language_model.layers.11.altup.prediction_coefs', 'model.language_model.layers.11.altup.modality_router', 'model.language_model.layers.11.per_layer_projection', 'model.language_model.layers.12.altup.correction_coefs', 'model.language_model.layers.12.altup.prediction_coefs', 'model.language_model.layers.12.altup.modality_router', 'model.language_model.layers.12.per_layer_projection', 'model.language_model.layers.13.altup.correction_coefs', 'model.language_model.layers.13.altup.prediction_coefs', 'model.language_model.layers.13.altup.modality_router', 'model.language_model.layers.13.per_layer_projection', 'model.language_model.layers.14.altup.correction_coefs', 'model.language_model.layers.14.altup.prediction_coefs', 'model.language_model.layers.14.altup.modality_router', 'model.language_model.layers.14.per_layer_projection', 'model.language_model.layers.15.altup.correction_coefs', 'model.language_model.layers.15.altup.prediction_coefs', 'model.language_model.layers.15.altup.modality_router', 'model.language_model.layers.15.per_layer_projection', 'model.language_model.layers.16.altup.correction_coefs', 'model.language_model.layers.16.altup.prediction_coefs', 'model.language_model.layers.16.altup.modality_router', 'model.language_model.layers.16.per_layer_projection', 'model.language_model.layers.17.altup.correction_coefs', 'model.language_model.layers.17.altup.prediction_coefs', 'model.language_model.layers.17.altup.modality_router', 'model.language_model.layers.17.per_layer_projection', 'model.language_model.layers.18.altup.correction_coefs', 'model.language_model.layers.18.altup.prediction_coefs', 'model.language_model.layers.18.altup.modality_router', 'model.language_model.layers.18.per_layer_projection', 'model.language_model.layers.19.altup.correction_coefs', 'model.language_model.layers.19.altup.prediction_coefs', 'model.language_model.layers.19.altup.modality_router', 'model.language_model.layers.19.per_layer_projection', 'model.language_model.layers.20.altup.correction_coefs', 'model.language_model.layers.20.altup.prediction_coefs', 'model.language_model.layers.20.altup.modality_router', 'model.language_model.layers.20.per_layer_projection', 'model.language_model.layers.21.altup.correction_coefs', 'model.language_model.layers.21.altup.prediction_coefs', 'model.language_model.layers.21.altup.modality_router', 'model.language_model.layers.21.per_layer_projection', 'model.language_model.layers.22.altup.correction_coefs', 'model.language_model.layers.22.altup.prediction_coefs', 'model.language_model.layers.22.altup.modality_router', 'model.language_model.layers.22.per_layer_projection', 'model.language_model.layers.23.altup.correction_coefs', 'model.language_model.layers.23.altup.prediction_coefs', 'model.language_model.layers.23.altup.modality_router', 'model.language_model.layers.23.per_layer_projection', 'model.language_model.layers.24.altup.correction_coefs', 'model.language_model.layers.24.altup.prediction_coefs', 'model.language_model.layers.24.altup.modality_router', 'model.language_model.layers.24.per_layer_projection', 'model.language_model.layers.25.altup.correction_coefs', 'model.language_model.layers.25.altup.prediction_coefs', 'model.language_model.layers.25.altup.modality_router', 'model.language_model.layers.25.per_layer_projection', 'model.language_model.layers.26.altup.correction_coefs', 'model.language_model.layers.26.altup.prediction_coefs', 'model.language_model.layers.26.altup.modality_router', 'model.language_model.layers.26.per_layer_projection', 'model.language_model.layers.27.altup.correction_coefs', 'model.language_model.layers.27.altup.prediction_coefs', 'model.language_model.layers.27.altup.modality_router', 'model.language_model.layers.27.per_layer_projection', 'model.language_model.layers.28.altup.correction_coefs', 'model.language_model.layers.28.altup.prediction_coefs', 'model.language_model.layers.28.altup.modality_router', 'model.language_model.layers.28.per_layer_projection', 'model.language_model.layers.29.altup.correction_coefs', 'model.language_model.layers.29.altup.prediction_coefs', 'model.language_model.layers.29.altup.modality_router', 'model.language_model.layers.29.per_layer_projection', 'model.language_model.layers.30.altup.correction_coefs', 'model.language_model.layers.30.altup.prediction_coefs', 'model.language_model.layers.30.altup.modality_router', 'model.language_model.layers.30.per_layer_projection', 'model.language_model.layers.31.altup.correction_coefs', 'model.language_model.layers.31.altup.prediction_coefs', 'model.language_model.layers.31.altup.modality_router', 'model.language_model.layers.31.per_layer_projection', 'model.language_model.layers.32.altup.correction_coefs', 'model.language_model.layers.32.altup.prediction_coefs', 'model.language_model.layers.32.altup.modality_router', 'model.language_model.layers.32.per_layer_projection', 'model.language_model.layers.33.altup.correction_coefs', 'model.language_model.layers.33.altup.prediction_coefs', 'model.language_model.layers.33.altup.modality_router', 'model.language_model.layers.33.per_layer_projection', 'model.language_model.layers.34.altup.correction_coefs', 'model.language_model.layers.34.altup.prediction_coefs', 'model.language_model.layers.34.altup.modality_router', 'model.language_model.layers.34.per_layer_projection', 'model.language_model.altup_projections.0', 'model.language_model.altup_projections.1', 'model.language_model.altup_projections.2', 'model.language_model.altup_unembed_projections.0', 'model.language_model.altup_unembed_projections.1', 'model.language_model.altup_unembed_projections.2', 'model.audio_tower.subsample_conv_projection.input_proj_linear', 'model.audio_tower.conformer.0.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.0.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.0.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.0.attention.attn.q_proj', 'model.audio_tower.conformer.0.attention.attn.k_proj', 'model.audio_tower.conformer.0.attention.attn.v_proj', 'model.audio_tower.conformer.0.attention.post', 'model.audio_tower.conformer.0.lconv1d.linear_start', 'model.audio_tower.conformer.0.lconv1d.linear_end', 'model.audio_tower.conformer.0.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.0.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.1.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.1.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.1.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.1.attention.attn.q_proj', 'model.audio_tower.conformer.1.attention.attn.k_proj', 'model.audio_tower.conformer.1.attention.attn.v_proj', 'model.audio_tower.conformer.1.attention.post', 'model.audio_tower.conformer.1.lconv1d.linear_start', 'model.audio_tower.conformer.1.lconv1d.linear_end', 'model.audio_tower.conformer.1.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.1.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.2.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.2.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.2.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.2.attention.attn.q_proj', 'model.audio_tower.conformer.2.attention.attn.k_proj', 'model.audio_tower.conformer.2.attention.attn.v_proj', 'model.audio_tower.conformer.2.attention.post', 'model.audio_tower.conformer.2.lconv1d.linear_start', 'model.audio_tower.conformer.2.lconv1d.linear_end', 'model.audio_tower.conformer.2.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.2.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.3.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.3.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.3.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.3.attention.attn.q_proj', 'model.audio_tower.conformer.3.attention.attn.k_proj', 'model.audio_tower.conformer.3.attention.attn.v_proj', 'model.audio_tower.conformer.3.attention.post', 'model.audio_tower.conformer.3.lconv1d.linear_start', 'model.audio_tower.conformer.3.lconv1d.linear_end', 'model.audio_tower.conformer.3.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.3.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.4.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.4.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.4.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.4.attention.attn.q_proj', 'model.audio_tower.conformer.4.attention.attn.k_proj', 'model.audio_tower.conformer.4.attention.attn.v_proj', 'model.audio_tower.conformer.4.attention.post', 'model.audio_tower.conformer.4.lconv1d.linear_start', 'model.audio_tower.conformer.4.lconv1d.linear_end', 'model.audio_tower.conformer.4.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.4.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.5.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.5.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.5.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.5.attention.attn.q_proj', 'model.audio_tower.conformer.5.attention.attn.k_proj', 'model.audio_tower.conformer.5.attention.attn.v_proj', 'model.audio_tower.conformer.5.attention.post', 'model.audio_tower.conformer.5.lconv1d.linear_start', 'model.audio_tower.conformer.5.lconv1d.linear_end', 'model.audio_tower.conformer.5.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.5.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.6.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.6.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.6.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.6.attention.attn.q_proj', 'model.audio_tower.conformer.6.attention.attn.k_proj', 'model.audio_tower.conformer.6.attention.attn.v_proj', 'model.audio_tower.conformer.6.attention.post', 'model.audio_tower.conformer.6.lconv1d.linear_start', 'model.audio_tower.conformer.6.lconv1d.linear_end', 'model.audio_tower.conformer.6.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.6.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.7.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.7.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.7.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.7.attention.attn.q_proj', 'model.audio_tower.conformer.7.attention.attn.k_proj', 'model.audio_tower.conformer.7.attention.attn.v_proj', 'model.audio_tower.conformer.7.attention.post', 'model.audio_tower.conformer.7.lconv1d.linear_start', 'model.audio_tower.conformer.7.lconv1d.linear_end', 'model.audio_tower.conformer.7.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.7.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.8.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.8.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.8.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.8.attention.attn.q_proj', 'model.audio_tower.conformer.8.attention.attn.k_proj', 'model.audio_tower.conformer.8.attention.attn.v_proj', 'model.audio_tower.conformer.8.attention.post', 'model.audio_tower.conformer.8.lconv1d.linear_start', 'model.audio_tower.conformer.8.lconv1d.linear_end', 'model.audio_tower.conformer.8.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.8.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.9.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.9.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.9.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.9.attention.attn.q_proj', 'model.audio_tower.conformer.9.attention.attn.k_proj', 'model.audio_tower.conformer.9.attention.attn.v_proj', 'model.audio_tower.conformer.9.attention.post', 'model.audio_tower.conformer.9.lconv1d.linear_start', 'model.audio_tower.conformer.9.lconv1d.linear_end', 'model.audio_tower.conformer.9.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.9.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.10.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.10.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.10.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.10.attention.attn.q_proj', 'model.audio_tower.conformer.10.attention.attn.k_proj', 'model.audio_tower.conformer.10.attention.attn.v_proj', 'model.audio_tower.conformer.10.attention.post', 'model.audio_tower.conformer.10.lconv1d.linear_start', 'model.audio_tower.conformer.10.lconv1d.linear_end', 'model.audio_tower.conformer.10.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.10.ffw_layer_end.ffw_layer_2', 'model.audio_tower.conformer.11.ffw_layer_start.ffw_layer_1', 'model.audio_tower.conformer.11.ffw_layer_start.ffw_layer_2', 'model.audio_tower.conformer.11.attention.attn.relative_position_embedding.pos_proj', 'model.audio_tower.conformer.11.attention.attn.q_proj', 'model.audio_tower.conformer.11.attention.attn.k_proj', 'model.audio_tower.conformer.11.attention.attn.v_proj', 'model.audio_tower.conformer.11.attention.post', 'model.audio_tower.conformer.11.lconv1d.linear_start', 'model.audio_tower.conformer.11.lconv1d.linear_end', 'model.audio_tower.conformer.11.ffw_layer_end.ffw_layer_1', 'model.audio_tower.conformer.11.ffw_layer_end.ffw_layer_2', 'model.embed_vision.embedding_projection', 'model.embed_audio.embedding_projection', 'lm_head']. Updating config.\nUnsloth: Saving merged 4bit model to ./saved_model...\nUnsloth: Merged 4bit model saved.\nUnsloth: Merged 4bit model process completed.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"GB_CONVERSION = 1024 ** 3\nSECONDS_TO_MINUTES = 60\n\n# Memory calculations\nused_memory_gb = torch.cuda.max_memory_reserved() / GB_CONVERSION\nused_memory_for_training_gb = used_memory_gb - start_gpu_memory\nused_percentage = (used_memory_gb / max_memory) * 100\ntraining_percentage = (used_memory_for_training_gb / max_memory) * 100\n\n# Time calculations\nruntime_seconds = trainer_stats.metrics['train_runtime']\nruntime_minutes = runtime_seconds / SECONDS_TO_MINUTES\n\nprint(\"TRAINING STATISTICS\")\nprint(\"=\" * 50)\nprint(f\"Training time: {runtime_seconds:.1f} seconds ({runtime_minutes:.2f} minutes)\")\nprint(f\"Peak memory usage: {used_memory_gb:.3f} GB ({used_percentage:.1f}% of max)\")\nprint(f\"Memory for training: {used_memory_for_training_gb:.3f} GB ({training_percentage:.1f}% of max)\")\nprint(\"=\" * 50)","metadata":{"id":"oclLlAuewltD","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:43:08.533510Z","iopub.execute_input":"2025-08-04T23:43:08.533780Z","iopub.status.idle":"2025-08-04T23:43:08.541476Z","shell.execute_reply.started":"2025-08-04T23:43:08.533757Z","shell.execute_reply":"2025-08-04T23:43:08.540518Z"}},"outputs":[{"name":"stdout","text":"TRAINING STATISTICS\n==================================================\nTraining time: 822.8 seconds (13.71 minutes)\nPeak memory usage: 12.592 GB (85.4% of max)\nMemory for training: -0.000 GB (-0.0% of max)\n==================================================\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import shutil\nfolder_path = \"./saved_model\"\nzip_path = f\"{folder_path}.zip\"\nshutil.make_archive(folder_path, 'zip', folder_path)\n\nfrom IPython.display import FileLink\nFileLink(zip_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T23:46:29.603425Z","iopub.execute_input":"2025-08-04T23:46:29.604148Z","iopub.status.idle":"2025-08-05T00:06:35.842199Z","shell.execute_reply.started":"2025-08-04T23:46:29.604107Z","shell.execute_reply":"2025-08-05T00:06:35.841057Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/saved_model.zip","text/html":"<a href='./saved_model.zip' target='_blank'>./saved_model.zip</a><br>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"[{\"role\": \"system\",\n                 \"content\": [{\"type\": \"text\", \"text\": model_instruction}]\n               }] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\n    \"role\": \"system\",\n    \"content\": [{\"type\": \"text\", \"text\": system_prompt_for_age(\"5-7\")}]\n    }, \n    {\n    \"role\": \"user\",\n    \"content\": [{\"type\": \"text\", \"text\": \"Why is the sky blue?\"}]\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:19:56.437591Z","iopub.execute_input":"2025-08-05T00:19:56.438478Z","iopub.status.idle":"2025-08-05T00:19:56.443372Z","shell.execute_reply.started":"2025-08-05T00:19:56.438443Z","shell.execute_reply":"2025-08-05T00:19:56.442633Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"do_gemma_3n_inference(model, messages, 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:20:00.513201Z","iopub.execute_input":"2025-08-05T00:20:00.514043Z","iopub.status.idle":"2025-08-05T00:21:15.893892Z","shell.execute_reply.started":"2025-08-05T00:20:00.514009Z","shell.execute_reply":"2025-08-05T00:21:15.892935Z"}},"outputs":[{"name":"stdout","text":"Hey there, superstar! ✨\n\nYou wanna know why the sky is blue? It's a super cool magic trick of the sun! ☀️\n\nThe sun's light is actually made of *all* the colors of the rainbow! 🌈 But when the sunlight comes to Earth, it bumps into tiny little bits in the air, like super-duper tiny sugar bugs! 🍬\n\nThese sugar bugs are really good at scattering the blue light *everywhere*! It's like they're playing a game of blue light tag! 💙\n\nThat's why when we look up, we see *mostly* blue! Isn't that amazing? 😄\n\n\n\n<end_of_turn>\n","output_type":"stream"}],"execution_count":26}]}