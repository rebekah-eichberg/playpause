{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12636371,"sourceType":"datasetVersion","datasetId":7984878},{"sourceId":12653101,"sourceType":"datasetVersion","datasetId":7996298}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:11:50.516622Z","iopub.execute_input":"2025-08-02T22:11:50.517045Z","iopub.status.idle":"2025-08-02T22:11:56.677735Z","shell.execute_reply.started":"2025-08-02T22:11:50.517018Z","shell.execute_reply":"2025-08-02T22:11:56.676622Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"%%capture\n# Install latest transformers for Gemma 3N\n!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:11:06.326210Z","iopub.execute_input":"2025-08-02T22:11:06.326555Z","iopub.status.idle":"2025-08-02T22:11:21.740364Z","shell.execute_reply.started":"2025-08-02T22:11:06.326518Z","shell.execute_reply":"2025-08-02T22:11:21.739562Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:11:21.741341Z","iopub.execute_input":"2025-08-02T22:11:21.741593Z","iopub.status.idle":"2025-08-02T22:11:21.746191Z","shell.execute_reply.started":"2025-08-02T22:11:21.741568Z","shell.execute_reply":"2025-08-02T22:11:21.745617Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:11:21.747679Z","iopub.execute_input":"2025-08-02T22:11:21.747890Z","iopub.status.idle":"2025-08-02T22:11:28.590685Z","shell.execute_reply.started":"2025-08-02T22:11:21.747870Z","shell.execute_reply":"2025-08-02T22:11:28.589750Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n    # Pretrained models\n    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n\n    # Other Gemma 3 quants\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3n-E4B-it\", # Or \"unsloth/gemma-3n-E2B-it\"\n    dtype = None, # None for auto detection\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:11:56.679424Z","iopub.execute_input":"2025-08-02T22:11:56.679706Z","iopub.status.idle":"2025-08-02T22:13:43.106582Z","shell.execute_reply.started":"2025-08-02T22:11:56.679681Z","shell.execute_reply":"2025-08-02T22:13:43.105903Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-02 22:12:11.165843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754172731.512435      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754172731.609455      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af1f64ac410d40d8975bab6edbe694a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a5baf7b3194d32b354d4049d51db1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ea277de1f54d5c80cdd7cf58de4e9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18409252f2c64dea9ebfa6df04bd3ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a555b0439434600a860e64a4b8f9aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"239d83fc78134269bdc09b2ed5b71af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383947ef12bd41fc91e83d78c3c31727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3260ee0399146bfa7b8a6b02a9eb124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380c4591023f49a3b7a0d2b6a248f941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92976339c4ba4f3f9e688aab52caa7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb8fe1984dd44afb5b04f7fe3347450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"168368f691494ef99e3c5251897a2505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f98c3e0bdf4156b559c771ce99db41"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import TextStreamer\nimport gc\n# Helper function for inference\ndef do_gemma_3n_inference(model, messages, max_new_tokens = 128):\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt = True, # Must add for generation\n        tokenize = True,\n        return_dict = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n    _ = model.generate(\n        **inputs,\n        max_new_tokens = max_new_tokens,\n        temperature = 1.0, top_p = 0.95, top_k = 64,\n        streamer = TextStreamer(tokenizer, skip_prompt = True),\n    )\n    # Cleanup to reduce VRAM usage\n    del inputs\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:43.107816Z","iopub.execute_input":"2025-08-02T22:13:43.108044Z","iopub.status.idle":"2025-08-02T22:13:43.113861Z","shell.execute_reply.started":"2025-08-02T22:13:43.108026Z","shell.execute_reply":"2025-08-02T22:13:43.113297Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, # Turn off for just text!\n    finetune_language_layers   = True,  # Should leave on!\n    finetune_attention_modules = True,  # Attention good for GRPO\n    finetune_mlp_modules       = True,  # Should leave on always!\n\n    r = 8,           # Larger = higher accuracy, but might overfit\n    lora_alpha = 8,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:43.114539Z","iopub.execute_input":"2025-08-02T22:13:43.114744Z","iopub.status.idle":"2025-08-02T22:13:53.166568Z","shell.execute_reply.started":"2025-08-02T22:13:43.114727Z","shell.execute_reply":"2025-08-02T22:13:53.165698Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model.language_model` require gradients\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:53.168741Z","iopub.execute_input":"2025-08-02T22:13:53.168984Z","iopub.status.idle":"2025-08-02T22:13:53.174139Z","shell.execute_reply.started":"2025-08-02T22:13:53.168965Z","shell.execute_reply":"2025-08-02T22:13:53.173408Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"%%capture \npip install python-docx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:53.174984Z","iopub.execute_input":"2025-08-02T22:13:53.175272Z","iopub.status.idle":"2025-08-02T22:13:56.795351Z","shell.execute_reply.started":"2025-08-02T22:13:53.175246Z","shell.execute_reply":"2025-08-02T22:13:56.794475Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# If not already installed in this environment; uncomment if needed\n# !pip install python-docx datasets transformers unsloth\n\nimport os\nimport re\nfrom collections import defaultdict\nfrom docx import Document\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\nfrom unsloth.chat_templates import get_chat_template\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:56.796662Z","iopub.execute_input":"2025-08-02T22:13:56.797382Z","iopub.status.idle":"2025-08-02T22:13:56.916931Z","shell.execute_reply.started":"2025-08-02T22:13:56.797351Z","shell.execute_reply":"2025-08-02T22:13:56.916421Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# === CONFIG ===\nINPUT_DIR = \"/kaggle/input/raw-docs\"  # folder with your .docx files\nAGE_GROUP_REGEX = re.compile(r\"Ages?\\s*([\\d\\-]+)\", flags=re.IGNORECASE)\nSPLIT_RATIOS = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E4B-it\"  # use this for testing; swap to your Gemma 3n checkpoint\nMAX_LENGTH = 256\nIGNORE_INDEX = -100\n\n# === Helpers ===\ndef infer_age_group_from_filename(path):\n    fname = os.path.basename(path)\n    m = AGE_GROUP_REGEX.search(fname)\n    return m.group(1) if m else \"unknown\"\n\ndef is_topic_header(text):\n    return bool(re.match(r\"^[A-Z][a-zA-Z& ]+$\", text)) and not text.endswith(\"?\")\n\ndef clean_answer(text):\n    text = text.replace(\"Sparky's Answer:\", \"\").strip()\n    return re.split(r\"Wow! Fact:.*?Wow!$\", text, flags=re.DOTALL)[0].strip()\n\ndef normalize_subject(subject):\n    if not subject:\n        return \"unknown\"\n    s = subject.strip().lower()\n    if \"math\" in s:\n        return \"math\"\n    if \"science\" in s:\n        return \"science\"\n    if \"geography\" in s:\n        return \"geography\"\n    if \"history\" in s:\n        return \"history\"\n    return s\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:56.917700Z","iopub.execute_input":"2025-08-02T22:13:56.918819Z","iopub.status.idle":"2025-08-02T22:13:56.926453Z","shell.execute_reply.started":"2025-08-02T22:13:56.918798Z","shell.execute_reply":"2025-08-02T22:13:56.925841Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def parse_docx_file(path):\n    print(f\"Parsing {path}\")\n    doc = Document(path)\n    age_group = infer_age_group_from_filename(path)\n    examples = []\n    current_topic = None\n    current_question = None\n    current_answer_parts = []\n\n    for para in doc.paragraphs:\n        text = para.text.strip()\n        if not text:\n            continue\n        if is_topic_header(text):\n            current_topic = text\n        elif text.endswith(\"?\") and \"Sparky's Answer\" not in text:\n            if current_question and current_answer_parts:\n                cleaned = clean_answer(\" \".join(current_answer_parts))\n                examples.append({\n                    \"question\": current_question,\n                    \"answer\": cleaned,\n                    \"subject\": normalize_subject(current_topic),\n                    \"age_group\": age_group,\n                    \"format\": \"open_ended\",\n                })\n                current_answer_parts = []\n            current_question = text\n        elif \"Sparky's Answer:\" in text or current_answer_parts:\n            current_answer_parts.append(text)\n\n    if current_question and current_answer_parts:\n        cleaned = clean_answer(\" \".join(current_answer_parts))\n        examples.append({\n            \"question\": current_question,\n            \"answer\": cleaned,\n            \"subject\": normalize_subject(current_topic),\n            \"age_group\": age_group,\n            \"format\": \"open_ended\",\n        })\n\n    print(f\"  â†’ extracted {len(examples)} examples\")\n    return examples\n\n# Collect all examples by age group\nby_age = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if fname.lower().endswith(\".docx\"):\n            path = os.path.join(root, fname)\n            exs = parse_docx_file(path)\n            for ex in exs:\n                by_age[ex[\"age_group\"]].append(ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:56.927314Z","iopub.execute_input":"2025-08-02T22:13:56.927532Z","iopub.status.idle":"2025-08-02T22:13:57.230763Z","shell.execute_reply.started":"2025-08-02T22:13:56.927515Z","shell.execute_reply":"2025-08-02T22:13:57.230037Z"}},"outputs":[{"name":"stdout","text":"Parsing /kaggle/input/raw-docs/Ages 5-7.docx\n  â†’ extracted 103 examples\nParsing /kaggle/input/raw-docs/Ages 1113.docx\n  â†’ extracted 49 examples\nParsing /kaggle/input/raw-docs/Ages 8-10.docx\n  â†’ extracted 206 examples\nParsing /kaggle/input/raw-docs/Ages 1415.docx\n  â†’ extracted 15 examples\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"raw_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\nif raw_tokenizer.pad_token is None:\n    raw_tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\nchat_tokenizer_wrapper = get_chat_template(raw_tokenizer, chat_template=\"gemma-3\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:13:57.231559Z","iopub.execute_input":"2025-08-02T22:13:57.231795Z","iopub.status.idle":"2025-08-02T22:14:00.830013Z","shell.execute_reply.started":"2025-08-02T22:13:57.231778Z","shell.execute_reply":"2025-08-02T22:14:00.829453Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945077f9e741451aa2c931e19102cc3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67639f73fcd46c1ab55bebcff4f364c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3128550c332d45f2aecf4f5e74512f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61dfbb6c62f943808f09ecdd7fe00797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86fdf54fef224f248496bef0ef14c92a"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def format_and_tokenize_batch(batch):\n    input_ids_list = []\n    labels_list = []\n    for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n        convo_full = [{\"role\":\"user\",\"content\":q}, {\"role\":\"assistant\",\"content\":a}]\n        convo_prompt = [{\"role\":\"user\",\"content\":q}]\n        full_text = chat_tokenizer_wrapper.apply_chat_template(\n            convo_full, tokenize=False, add_generation_prompt=False\n        ).removeprefix(\"<bos>\")\n        prompt_text = chat_tokenizer_wrapper.apply_chat_template(\n            convo_prompt, tokenize=False, add_generation_prompt=False\n        ).removeprefix(\"<bos>\")\n\n        tok_full = raw_tokenizer(full_text, truncation=True, max_length=MAX_LENGTH)\n        tok_prompt = raw_tokenizer(prompt_text, truncation=True, max_length=MAX_LENGTH)\n\n        input_ids = tok_full[\"input_ids\"]\n        labels = input_ids.copy()\n        prompt_len = len(tok_prompt[\"input_ids\"])\n        for i in range(prompt_len):\n            if i < len(labels):\n                labels[i] = IGNORE_INDEX\n        input_ids_list.append(input_ids)\n        labels_list.append(labels)\n\n    return {\"input_ids\": input_ids_list, \"labels\": labels_list}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:00.831822Z","iopub.execute_input":"2025-08-02T22:14:00.832043Z","iopub.status.idle":"2025-08-02T22:14:00.838088Z","shell.execute_reply.started":"2025-08-02T22:14:00.832025Z","shell.execute_reply":"2025-08-02T22:14:00.837513Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"processed_datasets = {}  # keep for inspection / training\n\nfor age_group, examples in by_age.items():\n    print(f\"\\n--- Age group {age_group} ({len(examples)} examples) ---\")\n    ds = Dataset.from_list(examples)\n\n    # If too few per subject for stratification, you can drop stratify_by_column temporarily\n    try:\n        train_testval = ds.train_test_split(\n            test_size=1 - SPLIT_RATIOS[\"train\"], seed=42, stratify_by_column=\"subject\"\n        )\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n            stratify_by_column=\"subject\"\n        )\n    except Exception as e:\n        print(\"Stratified split failed:\", e, \"Falling back to random split.\")\n        train_testval = ds.train_test_split(test_size=1 - SPLIT_RATIOS[\"train\"], seed=42)\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43\n        )\n\n    dataset_dict = DatasetDict({\n        \"train\": train_testval[\"train\"],\n        \"val\": val_test[\"train\"],\n        \"test\": val_test[\"test\"],\n    })\n\n    # Apply formatting+tokenization\n    def preprocess_split(split_ds):\n        return split_ds.map(\n            lambda batch: format_and_tokenize_batch(batch),\n            batched=True,\n            remove_columns=[c for c in split_ds.column_names if c not in (\"subject\",\"age_group\",\"format\")],\n        )\n\n    tokenized = DatasetDict({\n        split: preprocess_split(dataset_dict[split]) for split in [\"train\",\"val\",\"test\"]\n    })\n\n    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n    processed_datasets[age_group] = tokenized\n\n    # Optional: persist to disk per age group\n    out_dir = os.path.join(\"prepared_finetune_data_notebook\", f\"tokenized_age_{age_group}\")\n    tokenized.save_to_disk(out_dir)\n    print(f\"Saved tokenized dataset to {out_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:00.838831Z","iopub.execute_input":"2025-08-02T22:14:00.839032Z","iopub.status.idle":"2025-08-02T22:14:06.394167Z","shell.execute_reply.started":"2025-08-02T22:14:00.839016Z","shell.execute_reply":"2025-08-02T22:14:06.393355Z"}},"outputs":[{"name":"stdout","text":"\n--- Age group 5-7 (103 examples) ---\nStratified split failed: Stratifying by column is only supported for ClassLabel column, and column subject is Value. Falling back to random split.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"348e8b6769dd468ba1f1104bd1ef8a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0eb97ccfed747238ff0c8f38980c6ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca2140b353247f791b1b46c82e0b931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a3ded2d4fd48c886abfad174638118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44b84ea678d743c0a78358f1349f5263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b96bbe84bb43d29cf9b54de51b7895"}},"metadata":{}},{"name":"stdout","text":"Saved tokenized dataset to prepared_finetune_data_notebook/tokenized_age_5-7\n\n--- Age group 1113 (49 examples) ---\nStratified split failed: Stratifying by column is only supported for ClassLabel column, and column subject is Value. Falling back to random split.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e90d2f082cd4272991c64a5f40c28b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e627f862342f4bd5afa8baafbdeb941d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca53c7eed904e0e801ebc837f046f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/39 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c289aea28834c199c1cb05c32b52fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fcf12aae84d440eb2697fe313657259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b9c46429524a7abb9df80696b1868e"}},"metadata":{}},{"name":"stdout","text":"Saved tokenized dataset to prepared_finetune_data_notebook/tokenized_age_1113\n\n--- Age group 8-10 (206 examples) ---\nStratified split failed: Stratifying by column is only supported for ClassLabel column, and column subject is Value. Falling back to random split.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2323c7f3ba4f4702bb79209d99caae15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d721a0d2ee54fee9879dae96971d2ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ec658820d54b088b6129da5f8923d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79380bc4cfe2424ca39ad1079934bb6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09bd0efea7484334b2d3c0eb0667f7d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934b69523ede48079c077877ceca2572"}},"metadata":{}},{"name":"stdout","text":"Saved tokenized dataset to prepared_finetune_data_notebook/tokenized_age_8-10\n\n--- Age group 1415 (15 examples) ---\nStratified split failed: Stratifying by column is only supported for ClassLabel column, and column subject is Value. Falling back to random split.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d31edd48304d8f910e7cb86decff7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b961781b37f44f948c0eb917780b63b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0160977f8d4b639626a29616d60ea6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/12 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2421258dfe8d4dacbc74fee6b7825976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e8a3635a65d4bb39ead1f263f278960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e00e0545a7443d8e5aed7c0d19631c"}},"metadata":{}},{"name":"stdout","text":"Saved tokenized dataset to prepared_finetune_data_notebook/tokenized_age_1415\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from copy import deepcopy\n\ndef add_text_field(ds_dict, chat_tokenizer_wrapper):\n    # ds_dict is a DatasetDict with splits like \"train\",\"val\",\"test\", containing at least \"question\" and \"answer\"\n    def make_text(example):\n        convo = [\n            {\"role\": \"user\", \"content\": example[\"question\"]},\n            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n        ]\n        # generation prompt included so SFTTrainer knows where answer starts\n        example[\"text\"] = chat_tokenizer_wrapper.apply_chat_template(\n            convo, tokenize=False, add_generation_prompt=True\n        ).removeprefix(\"<bos>\")\n        return example\n\n    return ds_dict.map(make_text, batched=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:06.395170Z","iopub.execute_input":"2025-08-02T22:14:06.395477Z","iopub.status.idle":"2025-08-02T22:14:06.400296Z","shell.execute_reply.started":"2025-08-02T22:14:06.395451Z","shell.execute_reply":"2025-08-02T22:14:06.399511Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 1. Rebuild raw_datasets_by_age (stratified splits)\nby_age_raw = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if not fname.lower().endswith(\".docx\"):\n            continue\n        path = os.path.join(root, fname)\n        examples = parse_docx_file(path)  # your existing parser\n        for ex in examples:\n            by_age_raw[ex[\"age_group\"]].append(ex)\n\nraw_datasets_by_age = {}\nfor age_group, examples in by_age_raw.items():\n    ds = Dataset.from_list(examples)\n    try:\n        train_testval = ds.train_test_split(\n            test_size=1 - SPLIT_RATIOS[\"train\"],\n            seed=42,\n            stratify_by_column=\"subject\",\n        )\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n            stratify_by_column=\"subject\",\n        )\n    except Exception:\n        train_testval = ds.train_test_split(test_size=1 - SPLIT_RATIOS[\"train\"], seed=42)\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n        )\n\n    raw_datasets_by_age[age_group] = DatasetDict({\n        \"train\": train_testval[\"train\"],\n        \"val\": val_test[\"train\"],\n        \"test\": val_test[\"test\"],\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:06.401114Z","iopub.execute_input":"2025-08-02T22:14:06.401331Z","iopub.status.idle":"2025-08-02T22:14:06.707723Z","shell.execute_reply.started":"2025-08-02T22:14:06.401314Z","shell.execute_reply":"2025-08-02T22:14:06.707104Z"}},"outputs":[{"name":"stdout","text":"Parsing /kaggle/input/raw-docs/Ages 5-7.docx\n  â†’ extracted 103 examples\nParsing /kaggle/input/raw-docs/Ages 1113.docx\n  â†’ extracted 49 examples\nParsing /kaggle/input/raw-docs/Ages 8-10.docx\n  â†’ extracted 206 examples\nParsing /kaggle/input/raw-docs/Ages 1415.docx\n  â†’ extracted 15 examples\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 2. Add \"text\" field for SFTTrainer\ndef add_text_field(ds_dict):\n    def make_text(example):\n        convo = [\n            {\"role\": \"user\", \"content\": example[\"question\"]},\n            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n        ]\n        example[\"text\"] = chat_tokenizer_wrapper.apply_chat_template(\n            convo, tokenize=False, add_generation_prompt=True\n        ).removeprefix(\"<bos>\")\n        return example\n    return ds_dict.map(make_text, batched=False)\n\ndataset_texts_by_age = {}\nfor age_group, raw_ds in raw_datasets_by_age.items():\n    dataset_texts_by_age[age_group] = add_text_field(raw_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:06.708485Z","iopub.execute_input":"2025-08-02T22:14:06.708697Z","iopub.status.idle":"2025-08-02T22:14:11.770693Z","shell.execute_reply.started":"2025-08-02T22:14:06.708679Z","shell.execute_reply":"2025-08-02T22:14:11.769887Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b20eae4c0c54e71ae7ba7cb06a70517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f0e8fb5ad649ad8fdd85cce1043604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4658bccd7f154b7fb70e036b74b8011c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b863ac1518884857afa6fe74633a6578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a8cb28070f4f309d5d7590e3e76c47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093d57a8d4e04d5aa1db526229b95e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77470040037493b91384224bf3c86c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e05556489a340cc9941b6a1c34cc431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ebc6f9f9d142f989c9262d323cf59b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5df05edc9c4d88a19071582d9fb1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4de76a7b3e4e3481fcb23db7ea3cfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53014dc441df4716bc203307f5ad2829"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\nfrom trl import SFTTrainer, SFTConfig\nimport torch\nimport copy\n\n# Load base model once (frozen except adapters)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n    device_map=None\n).to(device)\n\nbase_model.eval()  # just to be safe; SFTTrainer will set train mode when needed\n\nadapter_checkpoints = {}\nprevious_adapter_path = None\n\n# Ensure age_order is sorted as desired\n# e.g., age_order = sorted(dataset_texts_by_age.keys(), key=lambda s: [int(x) for x in s.split(\"-\")])\nfor i, age in enumerate(age_order):\n    print(f\"\\n=== Curriculum step: age group {age} ===\")\n    ds = dataset_texts_by_age[age]  # has train/val/test with \"text\"\n\n    # Reload a fresh base model to avoid in-place contamination between adapters\n    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n\n    # LoRA config\n    peft_config = LoraConfig(\n        r=8,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],  # adjust if your model uses different module names\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    # Wrap base with LoRA\n    model = get_peft_model(model, peft_config)\n\n    # Warm-start from previous adapter if available\n    if previous_adapter_path:\n        # This assumes previous_adapter_path contains the PEFT adapter directory saved by trainer.save_model()\n        model = PeftModel.from_pretrained(model, previous_adapter_path)\n\n    model.print_trainable_parameters()\n\n    # Create SFTTrainer for this age\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=raw_tokenizer,\n        train_dataset=ds[\"train\"],\n        eval_dataset=ds.get(\"val\"),\n        args=SFTConfig(\n            dataset_text_field=\"text\",\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            warmup_steps=10,\n            max_steps=100,\n            learning_rate=2e-4,\n            logging_steps=10,\n            optim=\"paged_adamw_8bit\",\n            weight_decay=0.01,\n            lr_scheduler_type=\"linear\",\n            seed=42 + i,\n            report_to=\"none\",\n        ),\n    )\n\n    trainer.train()\n\n    # Save this age's adapter (only adapter weights + config)\n    save_dir = f\"./adapter_age_{age.replace('/', '_')}\"\n    trainer.save_model(save_dir)  # PEFT adapter checkpoint\n    adapter_checkpoints[age] = save_dir\n    previous_adapter_path = save_dir  # warm-start the next age from here\n\n    # Optional: cross-age validation on neighboring age\n    if i + 1 < len(age_order):\n        neighbor = age_order[i + 1]\n        print(f\"Evaluating adapter for age {age} on neighbor age {neighbor}'s val set:\")\n        neighbor_val = dataset_texts_by_age[neighbor][\"val\"]\n        eval_trainer = SFTTrainer(\n            model=model,  # current adapter loaded\n            tokenizer=raw_tokenizer,\n            train_dataset=None,\n            eval_dataset=neighbor_val,\n            args=SFTConfig(\n                dataset_text_field=\"text\",\n                per_device_train_batch_size=1,\n                gradient_accumulation_steps=1,\n                max_steps=1,\n                learning_rate=2e-4,\n                report_to=\"none\",\n            ),\n        )\n        print(eval_trainer.evaluate())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T22:14:11.771699Z","iopub.execute_input":"2025-08-02T22:14:11.772490Z","iopub.status.idle":"2025-08-02T22:16:31.521828Z","shell.execute_reply.started":"2025-08-02T22:14:11.772462Z","shell.execute_reply":"2025-08-02T22:16:31.520583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f730ce8b887a4cc09cf55228d04cdefa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039901c855584914b47835f92d35a80f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc94ecd42174c4a83557e1227b18912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ea857b2bb94b1c8c28bed6993be205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe8e78739764b248f69afc1ffdd9517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8fce4bfd4e4bf0b8a7df4c378e9a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90246db13334a54b3f930514518619e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae618a4f30ee4759845bb7a0ecde6dae"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2364861571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# just to be safe; SFTTrainer will set train mode when needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4274\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4275\u001b[0m                 )\n\u001b[0;32m-> 4276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 60.12 MiB is free. Process 5655 has 14.68 GiB memory in use. Of the allocated memory 14.51 GiB is allocated by PyTorch, and 38.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 60.12 MiB is free. Process 5655 has 14.68 GiB memory in use. Of the allocated memory 14.51 GiB is allocated by PyTorch, and 38.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":20},{"cell_type":"markdown","source":"NO LONGER NEED THE BELOW","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_data_formats\ndataset = standardize_data_formats(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n   convos = examples[\"messages\"]\n   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n   return { \"text\" : texts, }\n\ndataset = dataset.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[100][\"text\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the Model","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = None, # Can set up evaluation!\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n        logging_steps = 1,\n        optim = \"paged_adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<start_of_turn>user\\n\",\n    response_part = \"<start_of_turn>model\\n\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\" : \"text\",\n        \"text\" : \"How do birds know where to fly when they migrate?\",\n    }]\n}]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n    tokenize = True,\n    return_dict = True,\n).to(\"cuda\")\noutputs = model.generate(\n    **inputs,\n    max_new_tokens = 64, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"gemma-3n\")  # Local saving\ntokenizer.save_pretrained(\"gemma-3n\")\n# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}