{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:25:25.800890Z",
     "iopub.status.busy": "2025-08-06T04:25:25.800583Z",
     "iopub.status.idle": "2025-08-06T04:25:41.520708Z",
     "shell.execute_reply": "2025-08-06T04:25:41.519836Z",
     "shell.execute_reply.started": "2025-08-06T04:25:25.800861Z"
    },
    "id": "lR3QCP_OPx1g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:25:41.522973Z",
     "iopub.status.busy": "2025-08-06T04:25:41.522685Z",
     "iopub.status.idle": "2025-08-06T04:25:56.471581Z",
     "shell.execute_reply": "2025-08-06T04:25:56.470651Z",
     "shell.execute_reply.started": "2025-08-06T04:25:41.522920Z"
    },
    "id": "mdetXTiMPx1i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install latest transformers for Gemma 3N\n",
    "!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:25:56.473032Z",
     "iopub.status.busy": "2025-08-06T04:25:56.472700Z",
     "iopub.status.idle": "2025-08-06T04:25:56.477887Z",
     "shell.execute_reply": "2025-08-06T04:25:56.477000Z",
     "shell.execute_reply.started": "2025-08-06T04:25:56.472980Z"
    },
    "id": "9ELuVvaCPx1i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:26:00.735011Z",
     "iopub.status.busy": "2025-08-06T04:26:00.734598Z",
     "iopub.status.idle": "2025-08-06T04:27:37.778878Z",
     "shell.execute_reply": "2025-08-06T04:27:37.777957Z",
     "shell.execute_reply.started": "2025-08-06T04:26:00.734984Z"
    },
    "id": "jLv4h3zwPx1j",
    "outputId": "3c03ed8d-e23e-4211-e11d-7194ba284e5d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "\n",
    "BASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_NAME, \n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, \n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:27:37.780230Z",
     "iopub.status.busy": "2025-08-06T04:27:37.779880Z",
     "iopub.status.idle": "2025-08-06T04:27:37.787330Z",
     "shell.execute_reply": "2025-08-06T04:27:37.786548Z",
     "shell.execute_reply.started": "2025-08-06T04:27:37.780197Z"
    },
    "id": "KNv_IPQ5Px1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import gc\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(model, messages, max_new_tokens = 128):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )\n",
    "    # Cleanup to reduce VRAM usage\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:27:37.788401Z",
     "iopub.status.busy": "2025-08-06T04:27:37.788096Z",
     "iopub.status.idle": "2025-08-06T04:27:38.352040Z",
     "shell.execute_reply": "2025-08-06T04:27:38.351064Z",
     "shell.execute_reply.started": "2025-08-06T04:27:37.788373Z"
    },
    "id": "9LVb04YpPx1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:27:38.354995Z",
     "iopub.status.busy": "2025-08-06T04:27:38.354610Z",
     "iopub.status.idle": "2025-08-06T04:27:42.241543Z",
     "shell.execute_reply": "2025-08-06T04:27:42.240613Z",
     "shell.execute_reply.started": "2025-08-06T04:27:38.354966Z"
    },
    "id": "FmJJPH6QPx1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install python-docx\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from docx import Document\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:27:42.343734Z",
     "iopub.status.busy": "2025-08-06T04:27:42.342572Z",
     "iopub.status.idle": "2025-08-06T04:27:42.352454Z",
     "shell.execute_reply": "2025-08-06T04:27:42.351565Z",
     "shell.execute_reply.started": "2025-08-06T04:27:42.343710Z"
    },
    "id": "solStJ3EPx1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "INPUT_DIR = \"/kaggle/input/raw-docs\"  # folder with your .docx files\n",
    "AGE_GROUP_REGEX = re.compile(r\"Ages?\\s*([\\d\\-]+)\", flags=re.IGNORECASE)\n",
    "SPLIT_RATIOS = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n",
    "MAX_LENGTH = 256\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "# === Helpers ===\n",
    "def infer_age_group_from_filename(path):\n",
    "    fname = os.path.basename(path)\n",
    "    m = AGE_GROUP_REGEX.search(fname)\n",
    "    return m.group(1) if m else \"unknown\"\n",
    "\n",
    "def is_topic_header(text):\n",
    "    return bool(re.match(r\"^[A-Z][a-zA-Z& ]+$\", text)) and not text.endswith(\"?\")\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = text.replace(\"Sparky's Answer:\", \"\").strip()\n",
    "    return re.split(r\"Wow! Fact:.*?Wow!$\", text, flags=re.DOTALL)[0].strip()\n",
    "\n",
    "def normalize_subject(subject):\n",
    "    if not subject:\n",
    "        return \"unknown\"\n",
    "    s = subject.strip().lower()\n",
    "    if \"math\" in s:\n",
    "        return \"math\"\n",
    "    if \"science\" in s:\n",
    "        return \"science\"\n",
    "    if \"geography\" in s:\n",
    "        return \"geography\"\n",
    "    if \"history\" in s:\n",
    "        return \"history\"\n",
    "    return s\n",
    "def parse_docx_file(path):\n",
    "    print(f\"Parsing {path}\")\n",
    "    doc = Document(path)\n",
    "    age_group = infer_age_group_from_filename(path)\n",
    "    examples = []\n",
    "    current_topic = None\n",
    "    current_question = None\n",
    "    current_answer_parts = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        if is_topic_header(text):\n",
    "            current_topic = text\n",
    "        elif text.endswith(\"?\") and \"Sparky's Answer\" not in text:\n",
    "            if current_question and current_answer_parts:\n",
    "                cleaned = clean_answer(\" \".join(current_answer_parts))\n",
    "                examples.append({\n",
    "                    \"question\": current_question,\n",
    "                    \"answer\": cleaned,\n",
    "                    \"subject\": normalize_subject(current_topic),\n",
    "                    \"age_group\": age_group,\n",
    "                    \"format\": \"open_ended\",\n",
    "                })\n",
    "                current_answer_parts = []\n",
    "            current_question = text\n",
    "        elif \"Sparky's Answer:\" in text or current_answer_parts:\n",
    "            current_answer_parts.append(text)\n",
    "\n",
    "    if current_question and current_answer_parts:\n",
    "        cleaned = clean_answer(\" \".join(current_answer_parts))\n",
    "        examples.append({\n",
    "            \"question\": current_question,\n",
    "            \"answer\": cleaned,\n",
    "            \"subject\": normalize_subject(current_topic),\n",
    "            \"age_group\": age_group,\n",
    "            \"format\": \"open_ended\",\n",
    "        })\n",
    "\n",
    "    print(f\"  â†’ extracted {len(examples)} examples\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:27:42.353605Z",
     "iopub.status.busy": "2025-08-06T04:27:42.353243Z",
     "iopub.status.idle": "2025-08-06T04:27:42.692511Z",
     "shell.execute_reply": "2025-08-06T04:27:42.691607Z",
     "shell.execute_reply.started": "2025-08-06T04:27:42.353581Z"
    },
    "id": "_swfSYlXPx1m",
    "outputId": "c5e2c806-6594-4f7c-de34-e7c3794025b4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Collect all examples by age group\n",
    "by_age = defaultdict(list)\n",
    "for root, _, files in os.walk(INPUT_DIR):\n",
    "    for fname in files:\n",
    "        if fname.lower().endswith(\".docx\"):\n",
    "            path = os.path.join(root, fname)\n",
    "            exs = parse_docx_file(path)\n",
    "            for ex in exs:\n",
    "                by_age[ex[\"age_group\"]].append(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:07.001754Z",
     "iopub.status.busy": "2025-08-06T04:29:07.000883Z",
     "iopub.status.idle": "2025-08-06T04:29:07.381896Z",
     "shell.execute_reply": "2025-08-06T04:29:07.381002Z",
     "shell.execute_reply.started": "2025-08-06T04:29:07.001720Z"
    },
    "id": "1T3Y59fXPx1o",
    "outputId": "78b17900-fb2d-486f-9615-4f7d579f889c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build raw_datasets_by_age\n",
    "by_age_raw = defaultdict(list)\n",
    "for root, _, files in os.walk(INPUT_DIR):\n",
    "    for fname in files:\n",
    "        if not fname.lower().endswith(\".docx\"):\n",
    "            continue\n",
    "        path = os.path.join(root, fname)\n",
    "        examples = parse_docx_file(path)  # your existing parser\n",
    "        for ex in examples:\n",
    "            by_age_raw[ex[\"age_group\"]].append(ex)\n",
    "\n",
    "raw_datasets_by_age = {}\n",
    "for age_group, examples in by_age_raw.items():\n",
    "    ds = Dataset.from_list(examples)\n",
    "    try:\n",
    "        train_testval = ds.train_test_split(\n",
    "            test_size=1 - SPLIT_RATIOS[\"train\"],\n",
    "            seed=42,\n",
    "            stratify_by_column=\"subject\",\n",
    "        )\n",
    "        val_test = train_testval[\"test\"].train_test_split(\n",
    "            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n",
    "            seed=43,\n",
    "            stratify_by_column=\"subject\",\n",
    "        )\n",
    "        print(f\"-- Stratified succesfully for Age group {age_group}.\\n\")\n",
    "    except Exception:\n",
    "        train_testval = ds.train_test_split(test_size=1 - SPLIT_RATIOS[\"train\"], seed=42)\n",
    "        val_test = train_testval[\"test\"].train_test_split(\n",
    "            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n",
    "            seed=43,\n",
    "        )\n",
    "        print(f\"-- Stratification failed for Age group {age_group}.\\n\")\n",
    "\n",
    "    raw_datasets_by_age[age_group] = DatasetDict({\n",
    "        \"train\": train_testval[\"train\"],\n",
    "        \"val\": val_test[\"train\"],\n",
    "        \"test\": val_test[\"test\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:11.123677Z",
     "iopub.status.busy": "2025-08-06T04:29:11.123345Z",
     "iopub.status.idle": "2025-08-06T04:29:11.129115Z",
     "shell.execute_reply": "2025-08-06T04:29:11.128329Z",
     "shell.execute_reply.started": "2025-08-06T04:29:11.123645Z"
    },
    "id": "2iDhsxUKTCd3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def system_prompt_for_age(age_group):\n",
    "  if age_group == \"2-4\":\n",
    "    system_prompt = (\"You are a gentle and playful tutor for toddlers aged 2 to 4.\"\n",
    "        \"Use very short, simple words and sentences. Speak like a friendly character \"\n",
    "        \"from a children's show. Use repetition, sound effects, and lots of excitement!\")\n",
    "  elif age_group == \"5-7\":\n",
    "    system_prompt  = (\"You are a cheerful and friendly tutor for children aged 5 to 7.\"\n",
    "    \" Use simple words and fun metaphors to explain things clearly. Be playful and keep \"\n",
    "    \"answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' \"\n",
    "    \"to make it fun.\")\n",
    "  elif age_group == \"8-10\":\n",
    "    system_prompt  = (\"You are a smart and encouraging tutor for children aged 8 to 10.\"\n",
    "    \" Explain things using clear, age-appropriate language. Add interesting facts or \"\n",
    "    \"comparisons that make learning fun. You can use simple science words and \"\n",
    "    \"real-world examples.\")\n",
    "  elif age_group == \"1113\":\n",
    "    system_prompt  = (\"You are a knowledgeable and relatable tutor for preteens aged 11 to 13.\"\n",
    "    \" Use clear explanations and introduce scientific terms in an easy-to-understand way. \"\n",
    "    \"Be friendly and respectful, and encourage curiosity with slightly more detail.\")\n",
    "  elif age_group == \"1415\":\n",
    "    system_prompt  = (\"You are an insightful and respectful tutor for teenagers aged 14 to 15. \"\n",
    "    \"Use precise, informative language, and provide concise yet detailed explanations. \"\n",
    "    \"Speak like a cool, approachable mentor who respects their intelligence and encourages \"\n",
    "    \"critical thinking.\")\n",
    "  else:\n",
    "    system_prompt = (\"You are an insightful and respectful tutor for people above age 16.\")\n",
    "  return system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:16.448145Z",
     "iopub.status.busy": "2025-08-06T04:29:16.447488Z",
     "iopub.status.idle": "2025-08-06T04:29:22.556613Z",
     "shell.execute_reply": "2025-08-06T04:29:22.556053Z",
     "shell.execute_reply.started": "2025-08-06T04:29:16.448120Z"
    },
    "id": "lcc5AvU9Px1p",
    "outputId": "5f1fbc67-0c74-4b4d-f776-8f408ec365e6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add \"text\" field for SFTTrainer and tokenize\n",
    "def add_text_field(ds_dict, age_group):\n",
    "    def make_text(example):\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_for_age(age_group)},\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n",
    "        ]\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            convo, tokenize=False, add_generation_prompt=True\n",
    "        ).removeprefix(\"<bos>\")\n",
    "        return example\n",
    "    return ds_dict.map(make_text, batched=False)\n",
    "\n",
    "# Combine datasets from all age groups\n",
    "from datasets import concatenate_datasets \n",
    "combined_dataset = DatasetDict()\n",
    "for age_group, raw_ds in raw_datasets_by_age.items():\n",
    "    # Apply the modified add_text_field to each age group's dataset\n",
    "    processed_ds = add_text_field(raw_ds, age_group)\n",
    "    # Concatenate the splits\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        if split not in combined_dataset:\n",
    "            combined_dataset[split] = processed_ds[split]\n",
    "        else:\n",
    "            combined_dataset[split] = concatenate_datasets([combined_dataset[split], \n",
    "                                                            processed_ds[split]])\n",
    "\n",
    "ds = combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:23.870262Z",
     "iopub.status.busy": "2025-08-06T04:29:23.869632Z",
     "iopub.status.idle": "2025-08-06T04:29:23.875744Z",
     "shell.execute_reply": "2025-08-06T04:29:23.874991Z",
     "shell.execute_reply.started": "2025-08-06T04:29:23.870236Z"
    },
    "id": "5lUqnQOtt7IF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:29.421478Z",
     "iopub.status.busy": "2025-08-06T04:29:29.420801Z",
     "iopub.status.idle": "2025-08-06T04:29:44.727891Z",
     "shell.execute_reply": "2025-08-06T04:29:44.726981Z",
     "shell.execute_reply.started": "2025-08-06T04:29:29.421434Z"
    },
    "id": "tKQ96gjTPx1p",
    "outputId": "5a67acde-65bf-4fa8-96ce-fc30610d137c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Load base model once (frozen except adapters)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Add LoRA adapters to the model\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_cache = False,\n",
    "    use_gradient_checkpointing=True,  # True or \"unsloth\" for very long context\n",
    "    use_rslora=True,\n",
    "    random_state=73\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Create SFTTrainer for this age\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer= tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"val\"),\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:29:47.983638Z",
     "iopub.status.busy": "2025-08-06T04:29:47.982992Z",
     "iopub.status.idle": "2025-08-06T04:42:27.726385Z",
     "shell.execute_reply": "2025-08-06T04:42:27.725680Z",
     "shell.execute_reply.started": "2025-08-06T04:29:47.983606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:42:27.728500Z",
     "iopub.status.busy": "2025-08-06T04:42:27.727685Z",
     "iopub.status.idle": "2025-08-06T04:42:27.735064Z",
     "shell.execute_reply": "2025-08-06T04:42:27.734210Z",
     "shell.execute_reply.started": "2025-08-06T04:42:27.728479Z"
    },
    "id": "oclLlAuewltD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GB_CONVERSION = 1024 ** 3\n",
    "SECONDS_TO_MINUTES = 60\n",
    "\n",
    "# Memory calculations\n",
    "used_memory_gb = torch.cuda.max_memory_reserved() / GB_CONVERSION\n",
    "used_memory_for_training_gb = used_memory_gb - start_gpu_memory\n",
    "used_percentage = (used_memory_gb / max_memory) * 100\n",
    "training_percentage = (used_memory_for_training_gb / max_memory) * 100\n",
    "\n",
    "# Time calculations\n",
    "runtime_seconds = trainer_stats.metrics['train_runtime']\n",
    "runtime_minutes = runtime_seconds / SECONDS_TO_MINUTES\n",
    "\n",
    "print(\"TRAINING STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training time: {runtime_seconds:.1f} seconds ({runtime_minutes:.2f} minutes)\")\n",
    "print(f\"Peak memory usage: {used_memory_gb:.3f} GB ({used_percentage:.1f}% of max)\")\n",
    "print(f\"Memory for training: {used_memory_for_training_gb:.3f} GB ({training_percentage:.1f}% of max)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:42:27.736178Z",
     "iopub.status.busy": "2025-08-06T04:42:27.735890Z",
     "iopub.status.idle": "2025-08-06T04:44:11.317656Z",
     "shell.execute_reply": "2025-08-06T04:44:11.316924Z",
     "shell.execute_reply.started": "2025-08-06T04:42:27.736160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge lora adapters and save the fine-tuned model\n",
    "merged_model_save_path = f\"./merged_2B_model\"\n",
    "if not os.path.exists(merged_model_save_path):\n",
    "    os.makedirs(merged_model_save_path)\n",
    "model.save_pretrained_merged(merged_model_save_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T04:46:03.329745Z",
     "iopub.status.busy": "2025-08-06T04:46:03.329001Z",
     "iopub.status.idle": "2025-08-06T04:52:23.131288Z",
     "shell.execute_reply": "2025-08-06T04:52:23.129172Z",
     "shell.execute_reply.started": "2025-08-06T04:46:03.329710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Quantize and convert to gguf format\n",
    "model.save_pretrained_gguf(\n",
    "    merged_model_save_path,\n",
    "    quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8017082,
     "sourceId": 12686161,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
