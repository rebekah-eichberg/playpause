{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12686161,"sourceType":"datasetVersion","datasetId":8017082}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:25:25.800583Z","iopub.execute_input":"2025-08-06T04:25:25.800890Z","iopub.status.idle":"2025-08-06T04:25:41.520708Z","shell.execute_reply.started":"2025-08-06T04:25:25.800861Z","shell.execute_reply":"2025-08-06T04:25:41.519836Z"},"id":"lR3QCP_OPx1g"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Install latest transformers for Gemma 3N\n!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:25:41.522685Z","iopub.execute_input":"2025-08-06T04:25:41.522973Z","iopub.status.idle":"2025-08-06T04:25:56.471581Z","shell.execute_reply.started":"2025-08-06T04:25:41.522920Z","shell.execute_reply":"2025-08-06T04:25:56.470651Z"},"id":"mdetXTiMPx1i"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:25:56.472700Z","iopub.execute_input":"2025-08-06T04:25:56.473032Z","iopub.status.idle":"2025-08-06T04:25:56.477887Z","shell.execute_reply.started":"2025-08-06T04:25:56.472980Z","shell.execute_reply":"2025-08-06T04:25:56.477000Z"},"id":"9ELuVvaCPx1i"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:25:56.478650Z","iopub.execute_input":"2025-08-06T04:25:56.478985Z","iopub.status.idle":"2025-08-06T04:26:00.733902Z","shell.execute_reply.started":"2025-08-06T04:25:56.478953Z","shell.execute_reply":"2025-08-06T04:26:00.733213Z"},"id":"KjLMnqlMPx1j"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n    # Pretrained models\n    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n\n    # Other Gemma 3 quants\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it\"\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = BASE_MODEL_NAME, # Or \"unsloth/gemma-3n-E2B-it\"\n    dtype = None, # None for auto detection\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:26:00.734598Z","iopub.execute_input":"2025-08-06T04:26:00.735011Z","iopub.status.idle":"2025-08-06T04:27:37.778878Z","shell.execute_reply.started":"2025-08-06T04:26:00.734984Z","shell.execute_reply":"2025-08-06T04:27:37.777957Z"},"id":"jLv4h3zwPx1j","outputId":"3c03ed8d-e23e-4211-e11d-7194ba284e5d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\nimport gc\n# Helper function for inference\ndef do_gemma_3n_inference(model, messages, max_new_tokens = 128):\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt = True, # Must add for generation\n        tokenize = True,\n        return_dict = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n    _ = model.generate(\n        **inputs,\n        max_new_tokens = max_new_tokens,\n        temperature = 1.0, top_p = 0.95, top_k = 64,\n        streamer = TextStreamer(tokenizer, skip_prompt = True),\n    )\n    # Cleanup to reduce VRAM usage\n    del inputs\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:37.779880Z","iopub.execute_input":"2025-08-06T04:27:37.780230Z","iopub.status.idle":"2025-08-06T04:27:37.787330Z","shell.execute_reply.started":"2025-08-06T04:27:37.780197Z","shell.execute_reply":"2025-08-06T04:27:37.786548Z"},"id":"KNv_IPQ5Px1k"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:37.788096Z","iopub.execute_input":"2025-08-06T04:27:37.788401Z","iopub.status.idle":"2025-08-06T04:27:38.352040Z","shell.execute_reply.started":"2025-08-06T04:27:37.788373Z","shell.execute_reply":"2025-08-06T04:27:38.351064Z"},"id":"9LVb04YpPx1k"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\npip install python-docx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:38.354610Z","iopub.execute_input":"2025-08-06T04:27:38.354995Z","iopub.status.idle":"2025-08-06T04:27:42.241543Z","shell.execute_reply.started":"2025-08-06T04:27:38.354966Z","shell.execute_reply":"2025-08-06T04:27:42.240613Z"},"id":"FmJJPH6QPx1l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If not already installed in this environment; uncomment if needed\n# !pip install python-docx datasets transformers unsloth\n\nimport os\nimport re\nfrom collections import defaultdict\nfrom docx import Document\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:42.242754Z","iopub.execute_input":"2025-08-06T04:27:42.243103Z","iopub.status.idle":"2025-08-06T04:27:42.341692Z","shell.execute_reply.started":"2025-08-06T04:27:42.243074Z","shell.execute_reply":"2025-08-06T04:27:42.340972Z"},"id":"_mlxlMqBPx1l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CONFIG ===\nINPUT_DIR = \"/kaggle/input/raw-docs\"  # folder with your .docx files\nAGE_GROUP_REGEX = re.compile(r\"Ages?\\s*([\\d\\-]+)\", flags=re.IGNORECASE)\nSPLIT_RATIOS = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\nMAX_LENGTH = 256\nIGNORE_INDEX = -100\n\n# === Helpers ===\ndef infer_age_group_from_filename(path):\n    fname = os.path.basename(path)\n    m = AGE_GROUP_REGEX.search(fname)\n    return m.group(1) if m else \"unknown\"\n\ndef is_topic_header(text):\n    return bool(re.match(r\"^[A-Z][a-zA-Z& ]+$\", text)) and not text.endswith(\"?\")\n\ndef clean_answer(text):\n    text = text.replace(\"Sparky's Answer:\", \"\").strip()\n    return re.split(r\"Wow! Fact:.*?Wow!$\", text, flags=re.DOTALL)[0].strip()\n\ndef normalize_subject(subject):\n    if not subject:\n        return \"unknown\"\n    s = subject.strip().lower()\n    if \"math\" in s:\n        return \"math\"\n    if \"science\" in s:\n        return \"science\"\n    if \"geography\" in s:\n        return \"geography\"\n    if \"history\" in s:\n        return \"history\"\n    return s\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:42.342572Z","iopub.execute_input":"2025-08-06T04:27:42.343734Z","iopub.status.idle":"2025-08-06T04:27:42.352454Z","shell.execute_reply.started":"2025-08-06T04:27:42.343710Z","shell.execute_reply":"2025-08-06T04:27:42.351565Z"},"id":"solStJ3EPx1l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_docx_file(path):\n    print(f\"Parsing {path}\")\n    doc = Document(path)\n    age_group = infer_age_group_from_filename(path)\n    examples = []\n    current_topic = None\n    current_question = None\n    current_answer_parts = []\n\n    for para in doc.paragraphs:\n        text = para.text.strip()\n        if not text:\n            continue\n        if is_topic_header(text):\n            current_topic = text\n        elif text.endswith(\"?\") and \"Sparky's Answer\" not in text:\n            if current_question and current_answer_parts:\n                cleaned = clean_answer(\" \".join(current_answer_parts))\n                examples.append({\n                    \"question\": current_question,\n                    \"answer\": cleaned,\n                    \"subject\": normalize_subject(current_topic),\n                    \"age_group\": age_group,\n                    \"format\": \"open_ended\",\n                })\n                current_answer_parts = []\n            current_question = text\n        elif \"Sparky's Answer:\" in text or current_answer_parts:\n            current_answer_parts.append(text)\n\n    if current_question and current_answer_parts:\n        cleaned = clean_answer(\" \".join(current_answer_parts))\n        examples.append({\n            \"question\": current_question,\n            \"answer\": cleaned,\n            \"subject\": normalize_subject(current_topic),\n            \"age_group\": age_group,\n            \"format\": \"open_ended\",\n        })\n\n    print(f\"  → extracted {len(examples)} examples\")\n    return examples\n\n# Collect all examples by age group\nby_age = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if fname.lower().endswith(\".docx\"):\n            path = os.path.join(root, fname)\n            exs = parse_docx_file(path)\n            for ex in exs:\n                by_age[ex[\"age_group\"]].append(ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:27:42.353243Z","iopub.execute_input":"2025-08-06T04:27:42.353605Z","iopub.status.idle":"2025-08-06T04:27:42.692511Z","shell.execute_reply.started":"2025-08-06T04:27:42.353581Z","shell.execute_reply":"2025-08-06T04:27:42.691607Z"},"id":"_swfSYlXPx1m","outputId":"c5e2c806-6594-4f7c-de34-e7c3794025b4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build raw_datasets_by_age (stratified splits)\nby_age_raw = defaultdict(list)\nfor root, _, files in os.walk(INPUT_DIR):\n    for fname in files:\n        if not fname.lower().endswith(\".docx\"):\n            continue\n        path = os.path.join(root, fname)\n        examples = parse_docx_file(path)  # your existing parser\n        for ex in examples:\n            by_age_raw[ex[\"age_group\"]].append(ex)\n\nraw_datasets_by_age = {}\nfor age_group, examples in by_age_raw.items():\n    ds = Dataset.from_list(examples)\n    try:\n        train_testval = ds.train_test_split(\n            test_size=1 - SPLIT_RATIOS[\"train\"],\n            seed=42,\n            stratify_by_column=\"subject\",\n        )\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n            stratify_by_column=\"subject\",\n        )\n        print(f\"-- Stratified succesfully for Age group {age_group}.\\n\")\n    except Exception:\n        train_testval = ds.train_test_split(test_size=1 - SPLIT_RATIOS[\"train\"], seed=42)\n        val_test = train_testval[\"test\"].train_test_split(\n            test_size=SPLIT_RATIOS[\"test\"] / (SPLIT_RATIOS[\"val\"] + SPLIT_RATIOS[\"test\"]),\n            seed=43,\n        )\n        print(f\"-- Stratification failed for Age group {age_group}.\\n\")\n\n    raw_datasets_by_age[age_group] = DatasetDict({\n        \"train\": train_testval[\"train\"],\n        \"val\": val_test[\"train\"],\n        \"test\": val_test[\"test\"],\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:07.000883Z","iopub.execute_input":"2025-08-06T04:29:07.001754Z","iopub.status.idle":"2025-08-06T04:29:07.381896Z","shell.execute_reply.started":"2025-08-06T04:29:07.001720Z","shell.execute_reply":"2025-08-06T04:29:07.381002Z"},"id":"1T3Y59fXPx1o","outputId":"78b17900-fb2d-486f-9615-4f7d579f889c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def system_prompt_for_age(age_group):\n  if age_group == \"2-4\":\n    system_prompt = (\"You are a gentle and playful tutor for toddlers aged 2 to 4.\"\n        \"Use very short, simple words and sentences. Speak like a friendly character \"\n        \"from a children's show. Use repetition, sound effects, and lots of excitement!\")\n  elif age_group == \"5-7\":\n    system_prompt  = (\"You are a cheerful and friendly tutor for children aged 5 to 7.\"\n    \" Use simple words and fun metaphors to explain things clearly. Be playful and keep \"\n    \"answers short and exciting. You can use characters like 'sugar bugs' or 'energy monsters' \"\n    \"to make it fun.\")\n  elif age_group == \"8-10\":\n    system_prompt  = (\"You are a smart and encouraging tutor for children aged 8 to 10.\"\n    \" Explain things using clear, age-appropriate language. Add interesting facts or \"\n    \"comparisons that make learning fun. You can use simple science words and \"\n    \"real-world examples.\")\n  elif age_group == \"1113\":\n    system_prompt  = (\"You are a knowledgeable and relatable tutor for preteens aged 11 to 13.\"\n    \" Use clear explanations and introduce scientific terms in an easy-to-understand way. \"\n    \"Be friendly and respectful, and encourage curiosity with slightly more detail.\")\n  elif age_group == \"1415\":\n    system_prompt  = (\"You are an insightful and respectful tutor for teenagers aged 14 to 15. \"\n    \"Use precise, informative language, and provide concise yet detailed explanations. \"\n    \"Speak like a cool, approachable mentor who respects their intelligence and encourages \"\n    \"critical thinking.\")\n  else:\n    system_prompt = (\"You are an insightful and respectful tutor for people above age 16.\")\n  return system_prompt\n","metadata":{"id":"2iDhsxUKTCd3","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:11.123345Z","iopub.execute_input":"2025-08-06T04:29:11.123677Z","iopub.status.idle":"2025-08-06T04:29:11.129115Z","shell.execute_reply.started":"2025-08-06T04:29:11.123645Z","shell.execute_reply":"2025-08-06T04:29:11.128329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add \"text\" field for SFTTrainer\ndef add_text_field(ds_dict, age_group):\n    def make_text(example):\n        convo = [\n            {\"role\": \"system\", \"content\": system_prompt_for_age(age_group)},\n            {\"role\": \"user\", \"content\": example[\"question\"]},\n            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n        ]\n        example[\"text\"] = tokenizer.apply_chat_template(\n            convo, tokenize=False, add_generation_prompt=True\n        ).removeprefix(\"<bos>\")\n        return example\n    return ds_dict.map(make_text, batched=False)\n\n# Combine datasets from all age groups\nfrom datasets import concatenate_datasets # Import concatenate_datasets\n# Combine datasets from all age groups\ncombined_dataset = DatasetDict()\nfor age_group, raw_ds in raw_datasets_by_age.items():\n    # Apply the modified add_text_field to each age group's dataset\n    processed_ds = add_text_field(raw_ds, age_group)\n    # Concatenate the splits\n    for split in [\"train\", \"val\", \"test\"]:\n        if split not in combined_dataset:\n            combined_dataset[split] = processed_ds[split]\n        else:\n            combined_dataset[split] = concatenate_datasets([combined_dataset[split], processed_ds[split]])\n\nds = combined_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:16.447488Z","iopub.execute_input":"2025-08-06T04:29:16.448145Z","iopub.status.idle":"2025-08-06T04:29:22.556613Z","shell.execute_reply.started":"2025-08-06T04:29:16.448120Z","shell.execute_reply":"2025-08-06T04:29:22.556053Z"},"id":"lcc5AvU9Px1p","outputId":"5f1fbc67-0c74-4b4d-f776-8f408ec365e6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"id":"5lUqnQOtt7IF","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:23.869632Z","iopub.execute_input":"2025-08-06T04:29:23.870262Z","iopub.status.idle":"2025-08-06T04:29:23.875744Z","shell.execute_reply.started":"2025-08-06T04:29:23.870236Z","shell.execute_reply":"2025-08-06T04:29:23.874991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\nfrom trl import SFTTrainer, SFTConfig\nimport torch\nimport copy\n\n# Load base model once (frozen except adapters)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Add LoRA adapters to the model\nmodel = FastModel.get_peft_model(\n    model,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    bias=\"none\",\n    use_cache = False,\n    use_gradient_checkpointing=True,  # True or \"unsloth\" for very long context\n    use_rslora=True,\n    random_state=73\n)\n\nmodel.print_trainable_parameters()\n\n# Create SFTTrainer for this age\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer= tokenizer,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds.get(\"val\"),\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=10,\n        optim=\"paged_adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=42,\n        report_to=\"none\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:29.420801Z","iopub.execute_input":"2025-08-06T04:29:29.421478Z","iopub.status.idle":"2025-08-06T04:29:44.727891Z","shell.execute_reply.started":"2025-08-06T04:29:29.421434Z","shell.execute_reply":"2025-08-06T04:29:44.726981Z"},"id":"tKQ96gjTPx1p","outputId":"5a67acde-65bf-4fa8-96ce-fc30610d137c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import unsloth_train\ntrainer_stats = unsloth_train(trainer) # trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:29:47.982992Z","iopub.execute_input":"2025-08-06T04:29:47.983638Z","iopub.status.idle":"2025-08-06T04:42:27.726385Z","shell.execute_reply.started":"2025-08-06T04:29:47.983606Z","shell.execute_reply":"2025-08-06T04:42:27.725680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GB_CONVERSION = 1024 ** 3\nSECONDS_TO_MINUTES = 60\n\n# Memory calculations\nused_memory_gb = torch.cuda.max_memory_reserved() / GB_CONVERSION\nused_memory_for_training_gb = used_memory_gb - start_gpu_memory\nused_percentage = (used_memory_gb / max_memory) * 100\ntraining_percentage = (used_memory_for_training_gb / max_memory) * 100\n\n# Time calculations\nruntime_seconds = trainer_stats.metrics['train_runtime']\nruntime_minutes = runtime_seconds / SECONDS_TO_MINUTES\n\nprint(\"TRAINING STATISTICS\")\nprint(\"=\" * 50)\nprint(f\"Training time: {runtime_seconds:.1f} seconds ({runtime_minutes:.2f} minutes)\")\nprint(f\"Peak memory usage: {used_memory_gb:.3f} GB ({used_percentage:.1f}% of max)\")\nprint(f\"Memory for training: {used_memory_for_training_gb:.3f} GB ({training_percentage:.1f}% of max)\")\nprint(\"=\" * 50)","metadata":{"id":"oclLlAuewltD","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:42:27.727685Z","iopub.execute_input":"2025-08-06T04:42:27.728500Z","iopub.status.idle":"2025-08-06T04:42:27.735064Z","shell.execute_reply.started":"2025-08-06T04:42:27.728479Z","shell.execute_reply":"2025-08-06T04:42:27.734210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_model_save_path = f\"./merged_2B_model\"\n#cleanup_directory(model_dir)\nif not os.path.exists(merged_model_save_path):\n    os.makedirs(merged_model_save_path)\nmodel.save_pretrained_merged(merged_model_save_path, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:42:27.735890Z","iopub.execute_input":"2025-08-06T04:42:27.736178Z","iopub.status.idle":"2025-08-06T04:44:11.317656Z","shell.execute_reply.started":"2025-08-06T04:42:27.736160Z","shell.execute_reply":"2025-08-06T04:44:11.316924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"yes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:46:00.516865Z","iopub.execute_input":"2025-08-06T04:46:00.517194Z","iopub.status.idle":"2025-08-06T04:46:00.521872Z","shell.execute_reply.started":"2025-08-06T04:46:00.517172Z","shell.execute_reply":"2025-08-06T04:46:00.520915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained_gguf(\n    merged_model_save_path,\n    quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T04:46:03.329001Z","iopub.execute_input":"2025-08-06T04:46:03.329745Z","iopub.status.idle":"2025-08-06T04:52:23.131288Z","shell.execute_reply.started":"2025-08-06T04:46:03.329710Z","shell.execute_reply":"2025-08-06T04:52:23.129172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[{\"role\": \"system\",\n                 \"content\": [{\"type\": \"text\", \"text\": model_instruction}]\n               }] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\n    \"role\": \"system\",\n    \"content\": [{\"type\": \"text\", \"text\": system_prompt_for_age(\"5-7\")}]\n    }, \n    {\n    \"role\": \"user\",\n    \"content\": [{\"type\": \"text\", \"text\": \"Why is the sky blue?\"}]\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:19:56.437591Z","iopub.execute_input":"2025-08-05T00:19:56.438478Z","iopub.status.idle":"2025-08-05T00:19:56.443372Z","shell.execute_reply.started":"2025-08-05T00:19:56.438443Z","shell.execute_reply":"2025-08-05T00:19:56.442633Z"}},"outputs":[],"execution_count":null}]}